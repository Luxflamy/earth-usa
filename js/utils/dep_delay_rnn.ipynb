{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:16:16.902515Z",
     "start_time": "2025-04-17T23:16:16.887555Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import torch.nn.functional as F\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(2025)\n",
    "np.random.seed(2025)\n",
    "\n",
    "# Set device (CPU or GPU if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Utility function for JSON serialization\n",
    "def convert_to_serializable(obj):\n",
    "    \"\"\"\n",
    "    Convert NumPy types to Python native types for JSON serialization\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (np.integer, np.int64, np.int32, np.int16, np.int8)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32, np.float16)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.ndarray,)):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (pd.DataFrame,)):\n",
    "        return obj.to_dict('records')\n",
    "    elif isinstance(obj, (pd.Series,)):\n",
    "        return obj.to_dict()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_to_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Set data paths\n",
    "flight_data_path = './cleaned_data/'\n",
    "weather_data_path = './cleaned_weather_data/'\n",
    "top_airports_file = './top_100_airports.csv'  # File containing top 100 airports\n",
    "output_dir = './dep_delay_nn/'  # PyTorch ResNet output directory\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Starting year-by-year flight delay prediction models (RNN)...\")\n",
    "print(f\"Flight data directory: {flight_data_path}\")\n",
    "print(f\"Weather data directory: {weather_data_path}\")\n",
    "print(f\"Top airports file: {top_airports_file}\")\n",
    "print(f\"Model output directory: {output_dir}\")\n",
    "\n",
    "# Load top 30 airports from the top 100 airports file\n",
    "try:\n",
    "    # Load the airport data with the exact format provided\n",
    "    top_airports = pd.read_csv(top_airports_file, low_memory=False)\n",
    "    \n",
    "    # The file already has a Rank column, so we can just take the top 30\n",
    "    top_airports = top_airports.head(30)\n",
    "    \n",
    "    # The airport codes are in ORIGIN_IATA column\n",
    "    top_airport_codes = set(top_airports['ORIGIN_IATA'].str.strip().tolist())\n",
    "    \n",
    "    print(f\"Loaded top 30 airports: {', '.join(sorted(top_airport_codes))}\")\n",
    "    print(f\"Busiest airport: {top_airports.iloc[0]['ORIGIN_IATA']} with {top_airports.iloc[0]['Times']} flights\")\n",
    "    print(f\"30th busiest airport: {top_airports.iloc[29]['ORIGIN_IATA']} with {top_airports.iloc[29]['Times']} flights\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading top airports file: {e}\")\n",
    "    # Fallback: if file doesn't exist, we'll use all airports\n",
    "    top_airport_codes = None\n",
    "    print(\"Will process all airports (top airports file not available)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Starting year-by-year flight delay prediction models (RNN)...\n",
      "Flight data directory: ./cleaned_data/\n",
      "Weather data directory: ./cleaned_weather_data/\n",
      "Top airports file: ./top_100_airports.csv\n",
      "Model output directory: ./dep_delay_nn/\n",
      "Loaded top 30 airports: ATL, AUS, BNA, BOS, BWI, CLT, DCA, DEN, DFW, DTW, EWR, FLL, IAD, IAH, JFK, LAS, LAX, LGA, MCO, MDW, MIA, MSP, ORD, PHL, PHX, SAN, SEA, SFO, SLC, TPA\n",
      "Busiest airport: ATL with 457121 flights\n",
      "30th busiest airport: TPA with 97235 flights\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:16:16.919252Z",
     "start_time": "2025-04-17T23:16:16.902515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism layer to focus on important features\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, hidden_dim)\n",
    "        attention_weights = F.softmax(self.attention(x), dim=1)\n",
    "        # attention_weights shape: (batch_size, seq_len, 1)\n",
    "\n",
    "        context_vector = torch.sum(attention_weights * x, dim=1)\n",
    "        # context_vector shape: (batch_size, hidden_dim)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class RNNAttentionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Flight delay classification model based on RNN and attention mechanism\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=256, rnn_layers=2, dropout=0.3, bidirectional=True):\n",
    "        super(RNNAttentionClassifier, self).__init__()\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn_layers = rnn_layers\n",
    "\n",
    "        # Input feature embedding layer\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # RNN layer (LSTM)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=rnn_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if rnn_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = AttentionLayer(hidden_dim * 2 if bidirectional else hidden_dim)\n",
    "\n",
    "        # Prediction layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Apply embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Reshape for RNN (assuming we're dealing with single time step)\n",
    "        x = x.unsqueeze(1)  # becomes (batch_size, 1, hidden_dim)\n",
    "\n",
    "        # Apply RNN\n",
    "        self.rnn.flatten_parameters()\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        # rnn_out shape: (batch_size, 1, hidden_dim*2) if bidirectional\n",
    "\n",
    "        # Apply attention\n",
    "        context, attention_weights = self.attention(rnn_out)\n",
    "\n",
    "        # Predict delay probability\n",
    "        delay_prob = self.classifier(context)\n",
    "        return delay_prob\n",
    "\n",
    "class RNNAttentionRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Flight delay time prediction model based on RNN and attention mechanism\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=256, rnn_layers=2, dropout=0.3, bidirectional=True):\n",
    "        super(RNNAttentionRegressor, self).__init__()\n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn_layers = rnn_layers\n",
    "\n",
    "        # Input feature embedding layer\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        # RNN layer (GRU - may be better for time series)\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=rnn_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if rnn_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        # Attention layer\n",
    "        self.attention = AttentionLayer(hidden_dim * 2 if bidirectional else hidden_dim)\n",
    "\n",
    "        # Prediction layer\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Apply embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Reshape for RNN (assuming we're dealing with single time step)\n",
    "        x = x.unsqueeze(1)  # becomes (batch_size, 1, hidden_dim)\n",
    "\n",
    "        # Apply RNN\n",
    "        self.rnn.flatten_parameters()\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        # rnn_out shape: (batch_size, 1, hidden_dim*2) if bidirectional\n",
    "\n",
    "        # Apply attention\n",
    "        context, attention_weights = self.attention(rnn_out)\n",
    "\n",
    "        # Predict delay time\n",
    "        delay_time = self.regressor(context)\n",
    "        return delay_time\n",
    "\n",
    "# Custom Huber loss function to mitigate the impact of outliers\n",
    "class HuberLoss(nn.Module):\n",
    "    def __init__(self, delta=10.0):\n",
    "        super(HuberLoss, self).__init__()\n",
    "        self.delta = delta\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = y_pred.squeeze()\n",
    "        y_true = y_true.squeeze()\n",
    "        residual = torch.abs(y_pred - y_true)\n",
    "        condition = residual < self.delta\n",
    "        squared_loss = 0.5 * residual ** 2\n",
    "        linear_loss = self.delta * (residual - 0.5 * self.delta)\n",
    "        return torch.mean(torch.where(condition, squared_loss, linear_loss))\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:16:17.413566Z",
     "start_time": "2025-04-17T23:16:16.947199Z"
    }
   },
   "source": [
    "\n",
    "# Function to load weather data\n",
    "def load_weather_data():\n",
    "    print(\"\\nLoading weather data...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    all_files = glob.glob(os.path.join(weather_data_path, \"*.csv\"))\n",
    "    print(f\"Found {len(all_files)} total weather data files\")\n",
    "    weather_dict = {}\n",
    "    count = 0\n",
    "    matching_count = 0\n",
    "    \n",
    "    # Process all weather files\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            # Extract airport code and date information from filename\n",
    "            filename = os.path.basename(file)\n",
    "            parts = filename.split('.')[0].split('_')\n",
    "            \n",
    "            if len(parts) >= 3:\n",
    "                iata = parts[0]  # Airport code (e.g., ABI)\n",
    "                year = parts[1]  # Year (e.g., 2021)\n",
    "                month_name = parts[2]  # Month name (e.g., Aug)\n",
    "                \n",
    "                # Convert month name to number\n",
    "                month_map = {\n",
    "                    'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',\n",
    "                    'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "                    'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "                }\n",
    "                \n",
    "                if month_name in month_map:\n",
    "                    month = month_map[month_name]\n",
    "                    \n",
    "                    # Only continue with top airports if we have a list\n",
    "                    if top_airport_codes is None or iata in top_airport_codes:\n",
    "                        # Read the weather data\n",
    "                        weather_data = pd.read_csv(file, low_memory=False)\n",
    "                        \n",
    "                        # Ensure DATE column exists\n",
    "                        if 'DATE' not in weather_data.columns:\n",
    "                            print(f\"Warning: DATE column not found in {filename}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Convert DATE to datetime\n",
    "                        weather_data['DATE'] = pd.to_datetime(weather_data['DATE'])\n",
    "                        \n",
    "                        # Create the key for the weather dictionary\n",
    "                        key = f\"{iata}_{year}_{month}\"\n",
    "                        \n",
    "                        # Store the weather data\n",
    "                        weather_dict[key] = weather_data\n",
    "                        matching_count += 1\n",
    "                else:\n",
    "                    print(f\"Warning: Unknown month format in {filename}\")\n",
    "                \n",
    "                count += 1\n",
    "                \n",
    "                # Print progress periodically\n",
    "                if count % 100 == 0:\n",
    "                    print(f\"Processed {count} weather files, loaded {matching_count} matching files\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading weather file {file}: {e}\")\n",
    "    \n",
    "    print(f\"Loaded {matching_count} weather files out of {count} processed files\")\n",
    "    print(f\"Loading weather data took: {time.time() - start_time:.2f} seconds\")\n",
    "    return weather_dict\n",
    "\n",
    "# Get specific May files from the cleaned_data directory\n",
    "def get_may_files():\n",
    "    may_files = [\n",
    "        os.path.join(flight_data_path, \"May2021.csv\"),\n",
    "        os.path.join(flight_data_path, \"May2022.csv\"),\n",
    "        os.path.join(flight_data_path, \"May2023.csv\"),\n",
    "        os.path.join(flight_data_path, \"May2024.csv\")\n",
    "    ]\n",
    "    \n",
    "    # Verify each file exists\n",
    "    existing_files = []\n",
    "    for file_path in may_files:\n",
    "        if os.path.exists(file_path):\n",
    "            existing_files.append(file_path)\n",
    "        else:\n",
    "            print(f\"Warning: File {file_path} not found\")\n",
    "    \n",
    "    return existing_files\n",
    "\n",
    "# Get the May 2021-2024 flight data files\n",
    "flight_files = get_may_files()\n",
    "print(f\"\\nFound {len(flight_files)} May files to process:\")\n",
    "for f in flight_files:\n",
    "    print(f\"  - {os.path.basename(f)}\")\n",
    "\n",
    "if not flight_files:\n",
    "    print(\"No May 2021-2024 files were found. Please check file paths.\")\n",
    "    exit(1)\n",
    "\n",
    "# Load all weather data once\n",
    "weather_dict = load_weather_data()\n",
    "\n",
    "# Function to extract year from filename\n",
    "def extract_year_from_filename(filename):\n",
    "    # Extract year from 'May2021.csv', 'May2022.csv', etc.\n",
    "    base_name = os.path.basename(filename)\n",
    "    year_str = base_name.replace('May', '').split('.')[0]\n",
    "    return int(year_str)\n",
    "\n",
    "# Function to create red-eye flight indicator\n",
    "def create_redeye_indicator(df):\n",
    "    \"\"\"\n",
    "    Creates a binary indicator for red-eye flights (0-6 AM scheduled departure or arrival)\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing flight data with SCH_DEP_TIME and SCH_ARR_TIME\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with IS_REDEYE column added\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Initialize IS_REDEYE to 0 (not a red-eye flight)\n",
    "    df['IS_REDEYE'] = 0\n",
    "    \n",
    "    # Convert time columns to standard format if they exist\n",
    "    time_columns = []\n",
    "    \n",
    "    # Check for SCH_DEP_TIME\n",
    "    if 'SCH_DEP_TIME' in df.columns:\n",
    "        time_columns.append('SCH_DEP_TIME')\n",
    "    \n",
    "    # Check for SCH_ARR_TIME\n",
    "    if 'SCH_ARR_TIME' in df.columns:\n",
    "        time_columns.append('SCH_ARR_TIME')\n",
    "    \n",
    "    # Process each time column\n",
    "    for col in time_columns:\n",
    "        if df[col].dtype != 'float64':\n",
    "            try:\n",
    "                # Handle any non-numeric values\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            except:\n",
    "                print(f\"Warning: Could not convert {col} to numeric\")\n",
    "    \n",
    "    # Identify red-eye flights based on scheduled departure time (0-6 AM)\n",
    "    if 'SCH_DEP_TIME' in time_columns:\n",
    "        # Times are in HHMM format (e.g., 130 = 1:30 AM, 545 = 5:45 AM)\n",
    "        redeye_departure = (df['SCH_DEP_TIME'] >= 0) & (df['SCH_DEP_TIME'] < 600)\n",
    "        df.loc[redeye_departure, 'IS_REDEYE'] = 1\n",
    "        \n",
    "        # Count departures identified as red-eye\n",
    "        dep_redeye_count = redeye_departure.sum()\n",
    "        print(f\"Identified {dep_redeye_count} red-eye flights based on departure time (0-6 AM)\")\n",
    "    \n",
    "    # Identify red-eye flights based on scheduled arrival time (0-6 AM)\n",
    "    if 'SCH_ARR_TIME' in time_columns:\n",
    "        redeye_arrival = (df['SCH_ARR_TIME'] >= 0) & (df['SCH_ARR_TIME'] < 600)\n",
    "        df.loc[redeye_arrival, 'IS_REDEYE'] = 1\n",
    "        \n",
    "        # Count arrivals identified as red-eye\n",
    "        arr_redeye_count = redeye_arrival.sum()\n",
    "        print(f\"Identified {arr_redeye_count} red-eye flights based on arrival time (0-6 AM)\")\n",
    "    \n",
    "    # Print statistics about red-eye flights\n",
    "    redeye_count = df['IS_REDEYE'].sum()\n",
    "    total_count = len(df)\n",
    "    print(f\"Total identified red-eye flights: {redeye_count} out of {total_count} total flights ({redeye_count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    # Add a more detailed time-of-day categorical feature if needed\n",
    "    if 'SCH_DEP_TIME' in time_columns:\n",
    "        # Create a categorical time of day feature\n",
    "        df['DEP_TIME_OF_DAY'] = pd.cut(\n",
    "            df['SCH_DEP_TIME'], \n",
    "            bins=[0, 600, 1200, 1800, 2400],\n",
    "            labels=['Early Morning (0-6)', 'Morning (6-12)', 'Afternoon (12-18)', 'Evening (18-24)'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        \n",
    "        # Print distribution of flights by time of day\n",
    "        time_dist = df['DEP_TIME_OF_DAY'].value_counts()\n",
    "        print(\"\\nDistribution of flights by departure time of day:\")\n",
    "        for time_cat, count in time_dist.items():\n",
    "            print(f\"  - {time_cat}: {count} flights ({count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to prepare departure delay data\n",
    "def prepare_delay_data(df):\n",
    "    \"\"\"\n",
    "    Prepares departure delay data for modeling\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing flight data with DEP_DELAY column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with additional delay-related columns\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure DEP_DELAY is numeric\n",
    "    if 'DEP_DELAY' in df.columns:\n",
    "        if df['DEP_DELAY'].dtype != 'float64':\n",
    "            try:\n",
    "                df['DEP_DELAY'] = pd.to_numeric(df['DEP_DELAY'], errors='coerce')\n",
    "            except:\n",
    "                print(f\"Warning: Could not convert DEP_DELAY to numeric\")\n",
    "    else:\n",
    "        print(\"Warning: DEP_DELAY column not found in dataset\")\n",
    "        return df\n",
    "    \n",
    "    # Create a binary feature for on-time departure (<=0 means on time or early)\n",
    "    df['IS_DELAYED'] = (df['DEP_DELAY'] > 0).astype(int)\n",
    "    \n",
    "    # Create a categorical delay feature\n",
    "    df['DELAY_CATEGORY'] = pd.cut(\n",
    "        df['DEP_DELAY'],\n",
    "        bins=[-float('inf'), -15, 0, 15, 60, 120, float('inf')],\n",
    "        labels=['Very Early', 'Early', 'On Time', 'Moderate Delay',\n",
    "                'Significant Delay', 'Severe Delay'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Add absolute delay (for prediction error metrics)\n",
    "    df['ABS_DELAY'] = np.abs(df['DEP_DELAY'])\n",
    "    \n",
    "    # Print delay statistics\n",
    "    delay_count = df['IS_DELAYED'].sum()\n",
    "    total_count = len(df)\n",
    "    delay_rate = delay_count / total_count * 100\n",
    "    \n",
    "    print(f\"\\nDelay statistics:\")\n",
    "    print(f\"Delayed flights: {delay_count}/{total_count} ({delay_rate:.2f}%)\")\n",
    "    print(f\"On-time or early flights: {total_count - delay_count}/{total_count} ({100 - delay_rate:.2f}%)\")\n",
    "    \n",
    "    print(\"\\nDelay magnitude statistics:\")\n",
    "    print(f\"Mean delay: {df['DEP_DELAY'].mean():.2f} minutes\")\n",
    "    print(f\"Median delay: {df['DEP_DELAY'].median():.2f} minutes\")\n",
    "    print(f\"Min delay: {df['DEP_DELAY'].min():.2f} minutes (negative means early departure)\")\n",
    "    print(f\"Max delay: {df['DEP_DELAY'].max():.2f} minutes\")\n",
    "    \n",
    "    # Clip extreme values for neural network training\n",
    "    # This is important for neural networks as extreme outliers can cause training issues\n",
    "    upper_limit = df['DEP_DELAY'].quantile(0.995)  # 99.5th percentile\n",
    "    df['DEP_DELAY_CLIPPED'] = df['DEP_DELAY'].clip(upper=upper_limit)\n",
    "    \n",
    "    print(f\"Clipped delay values above {upper_limit:.2f} minutes for neural network training\")\n",
    "    print(f\"Number of clipped values: {(df['DEP_DELAY'] > upper_limit).sum()}\")\n",
    "    \n",
    "    # Print delay category distribution\n",
    "    delay_cat_dist = df['DELAY_CATEGORY'].value_counts()\n",
    "    print(\"\\nDelay category distribution:\")\n",
    "    for cat, count in delay_cat_dist.sort_index().items():\n",
    "        print(f\"  - {cat}: {count} flights ({count/total_count*100:.2f}%)\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to create advanced time features\n",
    "def create_advanced_time_features(df):\n",
    "    \"\"\"\n",
    "    Creates advanced time features including cyclic encoding, time blocks, and peak indicators\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing flight data with SCH_DEP_TIME\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with time features added\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    if 'SCH_DEP_TIME' not in df.columns:\n",
    "        print(\"Warning: SCH_DEP_TIME column not found for time features\")\n",
    "        return df\n",
    "    \n",
    "    # Ensure SCH_DEP_TIME is numeric\n",
    "    if df['SCH_DEP_TIME'].dtype != 'float64':\n",
    "        try:\n",
    "            df['SCH_DEP_TIME'] = pd.to_numeric(df['SCH_DEP_TIME'], errors='coerce')\n",
    "        except:\n",
    "            print(f\"Warning: Could not convert SCH_DEP_TIME to numeric\")\n",
    "            return df\n",
    "    \n",
    "    # Extract hour and minute from SCH_DEP_TIME (time format is HHMM)\n",
    "    df['DEP_HOUR'] = (df['SCH_DEP_TIME'] // 100).astype(int)\n",
    "    df['DEP_MINUTE'] = (df['SCH_DEP_TIME'] % 100).astype(int)\n",
    "    \n",
    "    # Calculate time in minutes from midnight\n",
    "    df['TIME_MINS'] = df['DEP_HOUR'] * 60 + df['DEP_MINUTE']\n",
    "    \n",
    "    # Create normalized time of day (0-1)\n",
    "    df['NORMALIZED_TIME'] = df['TIME_MINS'] / (24 * 60)\n",
    "    \n",
    "    # Create cyclic encodings at multiple frequencies\n",
    "    # 24-hour cycle\n",
    "    df['HOUR_SIN'] = np.sin(2 * np.pi * df['DEP_HOUR'] / 24)\n",
    "    df['HOUR_COS'] = np.cos(2 * np.pi * df['DEP_HOUR'] / 24)\n",
    "    \n",
    "    # 12-hour cycle (AM/PM pattern)\n",
    "    df['HALFDAY_SIN'] = np.sin(2 * np.pi * df['DEP_HOUR'] / 12)\n",
    "    df['HALFDAY_COS'] = np.cos(2 * np.pi * df['DEP_HOUR'] / 12)\n",
    "    \n",
    "    # 6-hour cycle (captures 4 parts of day)\n",
    "    df['QUARTER_DAY_SIN'] = np.sin(2 * np.pi * df['DEP_HOUR'] / 6)\n",
    "    df['QUARTER_DAY_COS'] = np.cos(2 * np.pi * df['DEP_HOUR'] / 6)\n",
    "    \n",
    "    # Create time blocks (each block is 3 hours)\n",
    "    time_blocks = {\n",
    "        0: 'Late Night (0-3)',\n",
    "        1: 'Late Night (0-3)',\n",
    "        2: 'Late Night (0-3)',\n",
    "        3: 'Early Morning (3-6)',\n",
    "        4: 'Early Morning (3-6)',\n",
    "        5: 'Early Morning (3-6)',\n",
    "        6: 'Morning (6-9)',\n",
    "        7: 'Morning (6-9)',\n",
    "        8: 'Morning (6-9)',\n",
    "        9: 'Mid-Day (9-12)',\n",
    "        10: 'Mid-Day (9-12)',\n",
    "        11: 'Mid-Day (9-12)',\n",
    "        12: 'Afternoon (12-15)',\n",
    "        13: 'Afternoon (12-15)',\n",
    "        14: 'Afternoon (12-15)',\n",
    "        15: 'Evening (15-18)',\n",
    "        16: 'Evening (15-18)',\n",
    "        17: 'Evening (15-18)',\n",
    "        18: 'Night (18-21)',\n",
    "        19: 'Night (18-21)',\n",
    "        20: 'Night (18-21)',\n",
    "        21: 'Late Night (21-24)',\n",
    "        22: 'Late Night (21-24)',\n",
    "        23: 'Late Night (21-24)'\n",
    "    }\n",
    "    \n",
    "    # Map hours to time blocks\n",
    "    df['TIME_BLOCK'] = df['DEP_HOUR'].map(time_blocks)\n",
    "    \n",
    "    # Create binary variables for peak times\n",
    "    # Morning peak (7-9 AM)\n",
    "    df['IS_MORNING_PEAK'] = ((df['DEP_HOUR'] >= 7) & (df['DEP_HOUR'] <= 9)).astype(int)\n",
    "    \n",
    "    # Evening peak (4-7 PM)\n",
    "    df['IS_EVENING_PEAK'] = ((df['DEP_HOUR'] >= 16) & (df['DEP_HOUR'] <= 19)).astype(int)\n",
    "    \n",
    "    # Create busy airport time indicators (combine time and hub status)\n",
    "    if 'IS_MAJOR_HUB_ORIGIN' in df.columns:\n",
    "        df['HUB_MORNING_PEAK'] = df['IS_MAJOR_HUB_ORIGIN'] * df['IS_MORNING_PEAK']\n",
    "        df['HUB_EVENING_PEAK'] = df['IS_MAJOR_HUB_ORIGIN'] * df['IS_EVENING_PEAK']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to create day of week features\n",
    "def create_advanced_day_features(df):\n",
    "    \"\"\"\n",
    "    Creates advanced day of week features with cyclic encoding\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing flight data with WEEK column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with day features added\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Check if we have the WEEK column with text day names\n",
    "    if 'WEEK' in df.columns:\n",
    "        # Create a mapping from abbreviated day names to full day names\n",
    "        day_name_map = {\n",
    "            'Sun': 'Sunday',\n",
    "            'Mon': 'Monday',\n",
    "            'Tue': 'Tuesday',\n",
    "            'Wed': 'Wednesday',\n",
    "            'Thu': 'Thursday',\n",
    "            'Fri': 'Friday',\n",
    "            'Sat': 'Saturday'\n",
    "        }\n",
    "        \n",
    "        # Map abbreviated names to full names\n",
    "        df['DAY_NAME'] = df['WEEK'].map(day_name_map)\n",
    "        \n",
    "        # Create weekend indicator\n",
    "        df['IS_WEEKEND'] = df['WEEK'].isin(['Sat', 'Sun']).astype(int)\n",
    "        \n",
    "        # Create day of week encoding - using cyclic encoding for better neural network performance\n",
    "        # This captures the cyclical nature of days of the week\n",
    "        # Convert day names to 0-6 numerical values (Monday=0, Sunday=6)\n",
    "        day_to_num = {'Mon': 0, 'Tue': 1, 'Wed': 2, 'Thu': 3, 'Fri': 4, 'Sat': 5, 'Sun': 6}\n",
    "        df['DAY_NUM'] = df['WEEK'].map(day_to_num)\n",
    "        \n",
    "        # Create cyclical features for day of week\n",
    "        # Weekly cycle\n",
    "        df['DAY_SIN'] = np.sin(2 * np.pi * df['DAY_NUM'] / 7)\n",
    "        df['DAY_COS'] = np.cos(2 * np.pi * df['DAY_NUM'] / 7)\n",
    "        \n",
    "        # Weekday/weekend cycle\n",
    "        df['WEEKDAY_SIN'] = np.sin(np.pi * df['IS_WEEKEND'])\n",
    "        df['WEEKDAY_COS'] = np.cos(np.pi * df['IS_WEEKEND'])\n",
    "        \n",
    "        # Create workweek features (5-day cycle for business days)\n",
    "        df['WORKWEEK_DAY'] = df['DAY_NUM'].apply(lambda x: x if x < 5 else np.nan)\n",
    "        # Fill weekend days with the mean of weekdays\n",
    "        work_day_mean = df['WORKWEEK_DAY'].mean()\n",
    "        df['WORKWEEK_DAY'] = df['WORKWEEK_DAY'].fillna(work_day_mean)\n",
    "        \n",
    "        # Workweek cycle\n",
    "        df['WORKWEEK_SIN'] = np.sin(2 * np.pi * df['WORKWEEK_DAY'] / 5)\n",
    "        df['WORKWEEK_COS'] = np.cos(2 * np.pi * df['WORKWEEK_DAY'] / 5)\n",
    "        \n",
    "        # Print distribution of days\n",
    "        day_counts = df['DAY_NAME'].value_counts()\n",
    "        total = len(df)\n",
    "        print(\"\\nDistribution of flights by day of week:\")\n",
    "        for day, count in day_counts.items():\n",
    "            print(f\"  - {day}: {count} flights ({count/total*100:.2f}%)\")\n",
    "        \n",
    "        # Print weekend vs. weekday distribution\n",
    "        weekend_count = df['IS_WEEKEND'].sum()\n",
    "        weekday_count = total - weekend_count\n",
    "        print(f\"\\nWeekend flights: {weekend_count} ({weekend_count/total*100:.2f}%)\")\n",
    "        print(f\"Weekday flights: {weekday_count} ({weekday_count/total*100:.2f}%)\")\n",
    "        \n",
    "    elif 'DAY_OF_WEEK' in df.columns:\n",
    "        # Assuming 1=Monday, ..., 7=Sunday or 0=Monday, ..., 6=Sunday\n",
    "        max_day = df['DAY_OF_WEEK'].max()\n",
    "        \n",
    "        if max_day == 7:\n",
    "            # 1-7 format (6,7 = weekend)\n",
    "            df['IS_WEEKEND'] = ((df['DAY_OF_WEEK'] == 6) | (df['DAY_OF_WEEK'] == 7)).astype(int)\n",
    "            \n",
    "            # Map day numbers to names for better interpretability\n",
    "            day_names = {1: 'Monday', 2: 'Tuesday', 3: 'Wednesday', \n",
    "                        4: 'Thursday', 5: 'Friday', 6: 'Saturday', 7: 'Sunday'}\n",
    "            \n",
    "            # For cyclic encoding, convert to 0-6 range\n",
    "            df['DAY_NUM'] = df['DAY_OF_WEEK'] - 1\n",
    "        else:\n",
    "            # 0-6 format (5,6 = weekend)\n",
    "            df['IS_WEEKEND'] = ((df['DAY_OF_WEEK'] == 5) | (df['DAY_OF_WEEK'] == 6)).astype(int)\n",
    "            \n",
    "            # Map day numbers to names for better interpretability\n",
    "            day_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', \n",
    "                        3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "            \n",
    "            # For cyclic encoding, we already have 0-6 range\n",
    "            df['DAY_NUM'] = df['DAY_OF_WEEK']\n",
    "        \n",
    "        df['DAY_NAME'] = df['DAY_OF_WEEK'].map(day_names)\n",
    "        \n",
    "        # Add cyclic encoding\n",
    "        df['DAY_SIN'] = np.sin(2 * np.pi * df['DAY_NUM'] / 7)\n",
    "        df['DAY_COS'] = np.cos(2 * np.pi * df['DAY_NUM'] / 7)\n",
    "        \n",
    "        # Weekday/weekend cycle\n",
    "        df['WEEKDAY_SIN'] = np.sin(np.pi * df['IS_WEEKEND'])\n",
    "        df['WEEKDAY_COS'] = np.cos(np.pi * df['IS_WEEKEND'])\n",
    "    else:\n",
    "        print(\"Warning: No day of week column (WEEK or DAY_OF_WEEK) found\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to create advanced airport features\n",
    "def create_airport_features(df):\n",
    "    \"\"\"\n",
    "    Creates advanced airport features\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing flight data with airport information\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with airport features added\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define major hub airports\n",
    "    major_hubs = ['ATL', 'DFW', 'DEN', 'ORD', 'LAX', 'CLT', 'LAS', 'PHX', 'MCO', 'SEA']\n",
    "    \n",
    "    # Create hub indicators\n",
    "    if 'ORIGIN_IATA' in df.columns:\n",
    "        df['IS_MAJOR_HUB_ORIGIN'] = df['ORIGIN_IATA'].isin(major_hubs).astype(int)\n",
    "    \n",
    "    if 'DEST_IATA' in df.columns:\n",
    "        df['IS_MAJOR_HUB_DEST'] = df['DEST_IATA'].isin(major_hubs).astype(int)\n",
    "    \n",
    "    # Create hub-to-hub flight indicator\n",
    "    if 'IS_MAJOR_HUB_ORIGIN' in df.columns and 'IS_MAJOR_HUB_DEST' in df.columns:\n",
    "        df['IS_HUB_TO_HUB'] = (df['IS_MAJOR_HUB_ORIGIN'] & df['IS_MAJOR_HUB_DEST']).astype(int)\n",
    "    \n",
    "    # Create region indicators (simplistic example)\n",
    "    if 'ORIGIN_IATA' in df.columns:\n",
    "        # West Coast airports\n",
    "        west_coast = ['LAX', 'SFO', 'SEA', 'PDX', 'SAN', 'LAS']\n",
    "        # East Coast airports\n",
    "        east_coast = ['JFK', 'LGA', 'EWR', 'BOS', 'DCA', 'IAD', 'MIA', 'FLL', 'ATL', 'CLT']\n",
    "        # Central/Midwest airports\n",
    "        central = ['ORD', 'MDW', 'DFW', 'IAH', 'DEN', 'MSP', 'DTW', 'STL']\n",
    "        \n",
    "        df['IS_WEST_COAST_ORIGIN'] = df['ORIGIN_IATA'].isin(west_coast).astype(int)\n",
    "        df['IS_EAST_COAST_ORIGIN'] = df['ORIGIN_IATA'].isin(east_coast).astype(int)\n",
    "        df['IS_CENTRAL_ORIGIN'] = df['ORIGIN_IATA'].isin(central).astype(int)\n",
    "        \n",
    "        df['IS_WEST_COAST_DEST'] = df['DEST_IATA'].isin(west_coast).astype(int)\n",
    "        df['IS_EAST_COAST_DEST'] = df['DEST_IATA'].isin(east_coast).astype(int)\n",
    "        df['IS_CENTRAL_DEST'] = df['DEST_IATA'].isin(central).astype(int)\n",
    "        \n",
    "        # Transcontinental flight indicator\n",
    "        df['IS_TRANSCON'] = ((df['IS_WEST_COAST_ORIGIN'] & df['IS_EAST_COAST_DEST']) | \n",
    "                             (df['IS_EAST_COAST_ORIGIN'] & df['IS_WEST_COAST_DEST'])).astype(int)\n",
    "    \n",
    "    # Create distance categories\n",
    "    if 'DISTANCE' in df.columns:\n",
    "        df['DISTANCE_CAT'] = pd.cut(\n",
    "            df['DISTANCE'], \n",
    "            bins=[0, 500, 1000, 1500, 2000, float('inf')],\n",
    "            labels=['Very Short', 'Short', 'Medium', 'Long', 'Very Long']\n",
    "        )\n",
    "        \n",
    "        # Create distance-based features for neural network\n",
    "        # Normalize distance\n",
    "        max_dist = df['DISTANCE'].max()\n",
    "        df['NORMALIZED_DISTANCE'] = df['DISTANCE'] / max_dist\n",
    "        \n",
    "        # Create logarithmic distance feature\n",
    "        df['LOG_DISTANCE'] = np.log1p(df['DISTANCE'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to create advanced weather features\n",
    "def create_weather_features(df):\n",
    "    \"\"\"\n",
    "    Creates advanced weather features\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing flight data with weather information\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with weather features added\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Check if we have the basic weather features\n",
    "    if 'PRCP' in df.columns:\n",
    "        # Create precipitation categories\n",
    "        df['RAIN_SEVERITY'] = pd.cut(\n",
    "            df['PRCP'],\n",
    "            bins=[-0.01, 0.0, 0.1, 0.5, 1.0, float('inf')],\n",
    "            labels=[0, 1, 2, 3, 4]\n",
    "        ).astype(int)\n",
    "    \n",
    "    # Combine weather features\n",
    "    if 'RAIN_SEVERITY' in df.columns and 'EXTREME_WEATHER' in df.columns:\n",
    "        df['WEATHER_SCORE'] = df['RAIN_SEVERITY'] + df['EXTREME_WEATHER'] * 3\n",
    "    \n",
    "    # Create weather interaction features\n",
    "    if 'IS_MAJOR_HUB_ORIGIN' in df.columns and 'WEATHER_SCORE' in df.columns:\n",
    "        df['HUB_WEATHER_IMPACT'] = df['IS_MAJOR_HUB_ORIGIN'] * df['WEATHER_SCORE']\n",
    "    \n",
    "    # Create time-weather interactions\n",
    "    if 'IS_MORNING_PEAK' in df.columns and 'WEATHER_SCORE' in df.columns:\n",
    "        df['PEAK_WEATHER_IMPACT'] = (df['IS_MORNING_PEAK'] | df['IS_EVENING_PEAK']) * df['WEATHER_SCORE']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to load and preprocess a single flight data file\n",
    "def load_and_process_flight_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess a single flight data file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the flight data file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with processed flight data\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {os.path.basename(file_path)}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Load flight data\n",
    "        df = pd.read_csv(file_path, low_memory=False)\n",
    "        original_size = len(df)\n",
    "        \n",
    "        # Extract year from filename\n",
    "        file_year = extract_year_from_filename(file_path)\n",
    "        \n",
    "        # Ensure the year is properly set\n",
    "        if 'YEAR' in df.columns:\n",
    "            # Verify that the year in the data matches the filename\n",
    "            unique_years = df['YEAR'].unique()\n",
    "            print(f\"Years found in data: {unique_years}\")\n",
    "            \n",
    "            # If data has multiple years, filter to only the year from filename\n",
    "            if len(unique_years) > 1:\n",
    "                df = df[df['YEAR'] == file_year]\n",
    "                print(f\"Filtered to only year {file_year}: {len(df)} rows\")\n",
    "        else:\n",
    "            # If no YEAR column exists, create one based on filename\n",
    "            df['YEAR'] = file_year\n",
    "            print(f\"Added YEAR column with value {file_year}\")\n",
    "        \n",
    "        # Ensure we only have May data\n",
    "        if 'MONTH' in df.columns:\n",
    "            month_counts = df['MONTH'].value_counts()\n",
    "            print(f\"Months found in data: {dict(month_counts)}\")\n",
    "            \n",
    "            if 5 in month_counts:\n",
    "                df = df[df['MONTH'] == 5]\n",
    "                print(f\"Filtered to only May data: {len(df)} rows\")\n",
    "            else:\n",
    "                print(f\"Warning: No May data found in file, but proceeding anyway as this should be May data based on filename\")\n",
    "        \n",
    "        # Check for DEP_DELAY column\n",
    "        if 'DEP_DELAY' not in df.columns:\n",
    "            print(f\"DEP_DELAY column not found in {os.path.basename(file_path)}. Skipping file.\")\n",
    "            return None\n",
    "        \n",
    "        # Filter for top airports if we have the list\n",
    "        if top_airport_codes is not None:\n",
    "            df = df[\n",
    "                df['ORIGIN_IATA'].str.strip().isin(top_airport_codes) & \n",
    "                df['DEST_IATA'].str.strip().isin(top_airport_codes)\n",
    "            ]\n",
    "            \n",
    "            filtered_size = len(df)\n",
    "            print(f\"Filtered from {original_size} to {filtered_size} rows for top 30 airports\")\n",
    "            \n",
    "            # If no data left after filtering, skip this file\n",
    "            if filtered_size == 0:\n",
    "                print(f\"No data remaining after filtering for top 30 airports. Skipping file.\")\n",
    "                return None\n",
    "        \n",
    "        # Remove cancelled flights since they don't have actual departure times\n",
    "        if 'CANCELLED' in df.columns:\n",
    "            cancelled_count = df['CANCELLED'].sum()\n",
    "            if cancelled_count > 0:\n",
    "                df = df[df['CANCELLED'] == 0]\n",
    "                print(f\"Removed {cancelled_count} cancelled flights, remaining: {len(df)}\")\n",
    "        \n",
    "        # Handle missing values in DEP_DELAY\n",
    "        if df['DEP_DELAY'].isnull().any():\n",
    "            missing_count = df['DEP_DELAY'].isnull().sum()\n",
    "            print(f\"Found {missing_count} rows with missing DEP_DELAY values. Removing them.\")\n",
    "            df = df.dropna(subset=['DEP_DELAY'])\n",
    "            print(f\"After removing rows with missing DEP_DELAY: {len(df)} rows\")\n",
    "        \n",
    "        print(f\"Processing took: {time.time() - start_time:.2f} seconds\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {os.path.basename(file_path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to match weather data to flights\n",
    "def match_weather_data(df):\n",
    "    \"\"\"\n",
    "    Match weather data to flight records\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing flight data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with weather data added\n",
    "    \"\"\"\n",
    "    print(\"\\nMatching weather data with flights...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Make sure necessary date columns exist\n",
    "    date_columns_exist = all(col in df.columns for col in ['YEAR', 'MONTH', 'DAY'])\n",
    "    if not date_columns_exist:\n",
    "        print(\"Warning: Missing one or more date columns (YEAR, MONTH, DAY)\")\n",
    "        print(\"Weather data cannot be matched\")\n",
    "        return df\n",
    "    \n",
    "    # Create a date column for matching - convert to datetime\n",
    "    df['FLIGHT_DATE'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']])\n",
    "    \n",
    "    # Create a column to hold the weather key pattern\n",
    "    df['WEATHER_KEY'] = df['ORIGIN_IATA'] + '_' + df['YEAR'].astype(str) + '_' + df['MONTH'].astype(str).str.zfill(2)\n",
    "    \n",
    "    # Create columns for weather features\n",
    "    weather_columns = ['EXTREME_WEATHER', 'PRCP', 'WT01', 'WT03', 'WT04', 'WT05', 'WT08', 'WT11']\n",
    "    for col in weather_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    \n",
    "    # Process in batches\n",
    "    matched_count = 0\n",
    "    batch_size = 10000\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        for idx, row in batch.iterrows():\n",
    "            try:\n",
    "                # Get the weather key based on the origin airport and date\n",
    "                weather_key = row['WEATHER_KEY']\n",
    "                flight_date = row['FLIGHT_DATE']\n",
    "                \n",
    "                # Check if this key exists in our weather dictionary\n",
    "                if weather_key in weather_dict:\n",
    "                    weather_data = weather_dict[weather_key]\n",
    "                    \n",
    "                    # Find matching weather data for the flight date\n",
    "                    matching_weather = weather_data[weather_data['DATE'] == flight_date]\n",
    "                    \n",
    "                    if not matching_weather.empty:\n",
    "                        # Match available weather columns\n",
    "                        for col in weather_columns:\n",
    "                            if col in matching_weather.columns:\n",
    "                                df.at[idx, col] = matching_weather[col].iloc[0]\n",
    "                        matched_count += 1\n",
    "            except Exception as e:\n",
    "                # Less verbose error reporting for speed\n",
    "                pass\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Processed {end_idx}/{len(df)} rows, matched {matched_count} flights with weather data\")\n",
    "    \n",
    "    print(f\"Matched weather data for {matched_count} flights ({matched_count/len(df)*100:.2f}%)\")\n",
    "    print(f\"Weather matching took: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return df"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 4 May files to process:\n",
      "  - May2021.csv\n",
      "  - May2022.csv\n",
      "  - May2023.csv\n",
      "  - May2024.csv\n",
      "\n",
      "Loading weather data...\n",
      "Found 3550 total weather data files\n",
      "Processed 100 weather files, loaded 0 matching files\n",
      "Processed 200 weather files, loaded 0 matching files\n",
      "Processed 300 weather files, loaded 16 matching files\n",
      "Processed 400 weather files, loaded 32 matching files\n",
      "Processed 500 weather files, loaded 32 matching files\n",
      "Processed 600 weather files, loaded 48 matching files\n",
      "Processed 700 weather files, loaded 48 matching files\n",
      "Processed 800 weather files, loaded 48 matching files\n",
      "Processed 900 weather files, loaded 64 matching files\n",
      "Processed 1000 weather files, loaded 64 matching files\n",
      "Processed 1100 weather files, loaded 80 matching files\n",
      "Processed 1200 weather files, loaded 96 matching files\n",
      "Processed 1300 weather files, loaded 112 matching files\n",
      "Processed 1400 weather files, loaded 112 matching files\n",
      "Processed 1500 weather files, loaded 112 matching files\n",
      "Processed 1600 weather files, loaded 112 matching files\n",
      "Processed 1700 weather files, loaded 128 matching files\n",
      "Processed 1800 weather files, loaded 128 matching files\n",
      "Processed 1900 weather files, loaded 143 matching files\n",
      "Processed 2000 weather files, loaded 160 matching files\n",
      "Processed 2100 weather files, loaded 160 matching files\n",
      "Processed 2200 weather files, loaded 192 matching files\n",
      "Processed 2300 weather files, loaded 208 matching files\n",
      "Processed 2400 weather files, loaded 208 matching files\n",
      "Processed 2500 weather files, loaded 224 matching files\n",
      "Processed 2600 weather files, loaded 240 matching files\n",
      "Processed 2700 weather files, loaded 240 matching files\n",
      "Processed 2800 weather files, loaded 256 matching files\n",
      "Processed 2900 weather files, loaded 256 matching files\n",
      "Processed 3000 weather files, loaded 272 matching files\n",
      "Processed 3100 weather files, loaded 286 matching files\n",
      "Processed 3200 weather files, loaded 306 matching files\n",
      "Processed 3300 weather files, loaded 320 matching files\n",
      "Processed 3400 weather files, loaded 336 matching files\n",
      "Processed 3500 weather files, loaded 336 matching files\n",
      "Loaded 336 weather files out of 3550 processed files\n",
      "Loading weather data took: 0.41 seconds\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:16:17.459791Z",
     "start_time": "2025-04-17T23:16:17.419764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Function to train RNN models\n",
    "def train_rnn_attention_models(X_train, X_test, y_train_class, y_test_class, y_train_reg, y_test_reg, year, output_dir, device):\n",
    "    \"\"\"\n",
    "    Build and train neural network models based on RNN+Attention\n",
    "\n",
    "    Args:\n",
    "        X_train: Training feature data\n",
    "        X_test: Test feature data\n",
    "        y_train_class: Training class labels\n",
    "        y_test_class: Test class labels\n",
    "        y_train_reg: Training regression values\n",
    "        y_test_reg: Test regression values\n",
    "        year: Year of the data\n",
    "        output_dir: Output directory for model and results\n",
    "        device: Device to use (CPU/GPU)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with model results and metrics\n",
    "    \"\"\"\n",
    "    print(\"\\nBuilding and training RNN+Attention neural network models...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create year-specific output directories\n",
    "    plots_dir = os.path.join(output_dir, f'plots_{year}')\n",
    "    models_dir = os.path.join(output_dir, f'models_{year}')\n",
    "    metrics_dir = os.path.join(output_dir, f'metrics_{year}')\n",
    "\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "    # Convert data to PyTorch tensors and create data loaders\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "\n",
    "    y_train_class_tensor = torch.FloatTensor(y_train_class.values).to(device)\n",
    "    y_test_class_tensor = torch.FloatTensor(y_test_class.values).to(device)\n",
    "\n",
    "    y_train_reg_tensor = torch.FloatTensor(y_train_reg.values).to(device)\n",
    "    y_test_reg_tensor = torch.FloatTensor(y_test_reg.values).to(device)\n",
    "\n",
    "    # Create datasets\n",
    "    train_class_dataset = TensorDataset(X_train_tensor, y_train_class_tensor)\n",
    "    test_class_dataset = TensorDataset(X_test_tensor, y_test_class_tensor)\n",
    "\n",
    "    train_reg_dataset = TensorDataset(X_train_tensor, y_train_reg_tensor)\n",
    "    test_reg_dataset = TensorDataset(X_test_tensor, y_test_reg_tensor)\n",
    "\n",
    "    # Create data loaders\n",
    "    batch_size = 1024  # Adjust based on available memory\n",
    "    train_class_loader = DataLoader(train_class_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_class_loader = DataLoader(test_class_dataset, batch_size=batch_size)\n",
    "\n",
    "    train_reg_loader = DataLoader(train_reg_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_reg_loader = DataLoader(test_reg_dataset, batch_size=batch_size)\n",
    "\n",
    "    # 1. Train Classification Model\n",
    "    print(\"\\nTraining RNN+Attention classification model...\")\n",
    "    class_model_start_time = time.time()\n",
    "\n",
    "    # Initialize model\n",
    "    input_dim = X_train.shape[1]\n",
    "    classifier = RNNAttentionClassifier(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=256,\n",
    "        rnn_layers=2,\n",
    "        dropout=0.3,\n",
    "        bidirectional=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion_class = nn.BCELoss()\n",
    "    optimizer_class = torch.optim.Adam(classifier.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler_class = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_class, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "\n",
    "    # Training loop parameters\n",
    "    num_epochs = 50\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Metrics tracking\n",
    "    train_losses_class = []\n",
    "    val_losses_class = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets in tqdm(train_class_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            # Zero the parameter gradients\n",
    "            optimizer_class.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = classifier(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion_class(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(classifier.parameters(), max_norm=1.0)\n",
    "            optimizer_class.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_class_dataset)\n",
    "        train_losses_class.append(epoch_train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        classifier.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_class_loader:\n",
    "                outputs = classifier(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion_class(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_val_loss = val_loss / len(test_class_dataset)\n",
    "        val_losses_class.append(epoch_val_loss)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler_class.step(epoch_val_loss)\n",
    "\n",
    "        # Print stats\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
    "\n",
    "        # Save best model\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_state = classifier.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    classifier.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(classifier.state_dict(), os.path.join(models_dir, f'rnn_attention_classifier_{year}.pth'))\n",
    "\n",
    "    class_model_training_time = time.time() - class_model_start_time\n",
    "    print(f\"Classification model training took: {class_model_training_time:.2f} seconds\")\n",
    "\n",
    "    # 2. Train Regression Model\n",
    "    print(\"\\nTraining RNN+Attention regression model...\")\n",
    "    reg_model_start_time = time.time()\n",
    "\n",
    "    # Initialize model\n",
    "    regressor = RNNAttentionRegressor(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=256,\n",
    "        rnn_layers=2,\n",
    "        dropout=0.3,\n",
    "        bidirectional=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Define loss function and optimizer (using custom Huber loss)\n",
    "    criterion_reg = HuberLoss(delta=15.0)\n",
    "    optimizer_reg = torch.optim.Adam(regressor.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler_reg = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_reg, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "\n",
    "    # Reset training loop parameters\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Metrics tracking\n",
    "    train_losses_reg = []\n",
    "    val_losses_reg = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        regressor.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets in tqdm(train_reg_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            # Zero the parameter gradients\n",
    "            optimizer_reg.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = regressor(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            loss = criterion_reg(outputs, targets)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(regressor.parameters(), max_norm=1.0)\n",
    "            optimizer_reg.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_reg_dataset)\n",
    "        train_losses_reg.append(epoch_train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        regressor.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_reg_loader:\n",
    "                outputs = regressor(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criterion_reg(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_val_loss = val_loss / len(test_reg_dataset)\n",
    "        val_losses_reg.append(epoch_val_loss)\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        scheduler_reg.step(epoch_val_loss)\n",
    "\n",
    "        # Print stats\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
    "\n",
    "        # Save best model\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_state = regressor.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    regressor.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(regressor.state_dict(), os.path.join(models_dir, f'rnn_attention_regressor_{year}.pth'))\n",
    "\n",
    "    reg_model_training_time = time.time() - reg_model_start_time\n",
    "    print(f\"Regression model training took: {reg_model_training_time:.2f} seconds\")\n",
    "\n",
    "    # 3. Evaluate Classification Model\n",
    "    print(\"\\nEvaluating classification model...\")\n",
    "\n",
    "    classifier.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_class_loader:\n",
    "            outputs = classifier(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "\n",
    "            probs = outputs.cpu().numpy()\n",
    "            preds = (outputs >= 0.5).float().cpu().numpy()\n",
    "\n",
    "            all_probs.extend(probs)\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Calculate metrics\n",
    "    class_accuracy = (all_preds == all_targets).mean() * 100\n",
    "\n",
    "    # Check if there are enough classes for ROC/AUC evaluation\n",
    "    if len(np.unique(all_targets)) > 1:\n",
    "        class_roc_auc = roc_auc_score(all_targets, all_probs)\n",
    "    else:\n",
    "        print(\"Warning: Only one class present in test set. ROC AUC score cannot be calculated.\")\n",
    "        class_roc_auc = 0.0\n",
    "\n",
    "    class_report = classification_report(all_targets, all_preds, output_dict=True)\n",
    "    class_cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "    print(f\"Classification Accuracy: {class_accuracy:.2f}%\")\n",
    "    print(f\"Classification ROC AUC: {class_roc_auc:.4f}\")\n",
    "\n",
    "    # Check if '1' key exists in classification report\n",
    "    if '1' in class_report:\n",
    "        print(f\"Classification Precision (Delayed): {class_report['1']['precision']:.4f}\")\n",
    "        print(f\"Classification Recall (Delayed): {class_report['1']['recall']:.4f}\")\n",
    "        print(f\"Classification F1 Score (Delayed): {class_report['1']['f1-score']:.4f}\")\n",
    "        precision = class_report['1']['precision']\n",
    "        recall = class_report['1']['recall']\n",
    "        f1_score = class_report['1']['f1-score']\n",
    "    else:\n",
    "        print(\"Warning: Class '1' not present in test results. Using default metrics.\")\n",
    "        precision = 0.0\n",
    "        recall = 0.0\n",
    "        f1_score = 0.0\n",
    "\n",
    "    # 4. Evaluate Regression Model\n",
    "    print(\"\\nEvaluating regression model...\")\n",
    "\n",
    "    regressor.eval()\n",
    "    all_reg_preds = []\n",
    "    all_reg_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_reg_loader:\n",
    "            outputs = regressor(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "\n",
    "            all_reg_preds.extend(outputs.cpu().numpy())\n",
    "            all_reg_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    all_reg_preds = np.array(all_reg_preds)\n",
    "    all_reg_targets = np.array(all_reg_targets)\n",
    "\n",
    "    # Calculate metrics\n",
    "    reg_mse = mean_squared_error(all_reg_targets, all_reg_preds)\n",
    "    reg_rmse = np.sqrt(reg_mse)\n",
    "    reg_mae = mean_absolute_error(all_reg_targets, all_reg_preds)\n",
    "    reg_r2 = r2_score(all_reg_targets, all_reg_preds)\n",
    "\n",
    "    print(f\"Regression Mean Squared Error: {reg_mse:.2f}\")\n",
    "    print(f\"Regression Root Mean Squared Error: {reg_rmse:.2f} minutes\")\n",
    "    print(f\"Regression Mean Absolute Error: {reg_mae:.2f} minutes\")\n",
    "    print(f\"Regression R Score: {reg_r2:.4f}\")\n",
    "\n",
    "    # 5. Create plots and visualizations\n",
    "\n",
    "    # Plot training/validation loss for classification\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses_class, label='Train Loss')\n",
    "    plt.plot(val_losses_class, label='Validation Loss')\n",
    "    plt.title(f'Classification Loss Curves - {year}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training/validation loss for regression\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_losses_reg, label='Train Loss')\n",
    "    plt.plot(val_losses_reg, label='Validation Loss')\n",
    "    plt.title(f'Regression Loss Curves - {year}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'rnn_learning_curves_{year}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot confusion matrix for classification\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    sns.heatmap(class_cm, annot=True, fmt='d', cmap='Blues',\n",
    "               xticklabels=['Not Delayed', 'Delayed'],\n",
    "               yticklabels=['Not Delayed', 'Delayed'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Delay Classification Confusion Matrix ({year})')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'rnn_confusion_matrix_{year}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Check if ROC curve can be plotted\n",
    "    if len(np.unique(all_targets)) > 1:\n",
    "        # Plot ROC curve for classification\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        fpr, tpr, _ = roc_curve(all_targets, all_probs)\n",
    "        plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {class_roc_auc:.4f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve for Delay Classification ({year})')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(plots_dir, f'rnn_roc_curve_{year}.png'))\n",
    "        plt.close()\n",
    "\n",
    "    # Plot actual vs predicted delays for regression\n",
    "    plt.figure(figsize=(16, 10))\n",
    "\n",
    "    # Create a scatterplot with limited points for clarity\n",
    "    max_points = 5000\n",
    "    if len(all_reg_targets) > max_points:\n",
    "        idx = np.random.choice(len(all_reg_targets), max_points, replace=False)\n",
    "        sample_actual = all_reg_targets[idx]\n",
    "        sample_pred = all_reg_preds[idx]\n",
    "    else:\n",
    "        sample_actual = all_reg_targets\n",
    "        sample_pred = all_reg_preds\n",
    "\n",
    "    plt.scatter(sample_actual, sample_pred, alpha=0.3)\n",
    "\n",
    "    # Add perfect prediction line\n",
    "    max_val = max(np.max(sample_actual), np.max(sample_pred))\n",
    "    min_val = min(np.min(sample_actual), np.min(sample_pred))\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "\n",
    "    plt.xlabel('Actual Delay (minutes)')\n",
    "    plt.ylabel('Predicted Delay (minutes)')\n",
    "    plt.title(f'Actual vs Predicted Delay ({year})')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'rnn_actual_vs_predicted_{year}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot delay prediction error distribution\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    prediction_errors = all_reg_targets - all_reg_preds\n",
    "    sns.histplot(prediction_errors, bins=50, kde=True)\n",
    "    plt.axvline(0, color='red', linestyle='--')\n",
    "    plt.xlabel('Prediction Error (minutes)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Delay Prediction Error Distribution ({year})')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, f'rnn_error_distribution_{year}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Return metrics\n",
    "    metrics = {\n",
    "        'model_name': f'rnn_attention_{year}',\n",
    "        'year': year,\n",
    "\n",
    "        # Classification metrics\n",
    "        'class_accuracy': class_accuracy,\n",
    "        'class_roc_auc': class_roc_auc,\n",
    "        'class_precision': precision,\n",
    "        'class_recall': recall,\n",
    "        'class_f1': f1_score,\n",
    "        'class_training_time': class_model_training_time,\n",
    "        'class_epochs': len(train_losses_class),\n",
    "\n",
    "        # Regression metrics\n",
    "        'reg_mse': reg_mse,\n",
    "        'reg_rmse': reg_rmse,\n",
    "        'reg_mae': reg_mae,\n",
    "        'reg_r2': reg_r2,\n",
    "        'reg_training_time': reg_model_training_time,\n",
    "        'reg_epochs': len(train_losses_reg),\n",
    "\n",
    "        # Confusion matrix values\n",
    "        'true_negative': int(class_cm[0, 0]) if class_cm.shape == (2, 2) else 0,\n",
    "        'false_positive': int(class_cm[0, 1]) if class_cm.shape == (2, 2) else 0,\n",
    "        'false_negative': int(class_cm[1, 0]) if class_cm.shape == (2, 2) else 0,\n",
    "        'true_positive': int(class_cm[1, 1]) if class_cm.shape == (2, 2) else 0,\n",
    "\n",
    "        'status': 'success',\n",
    "        'total_processing_time': time.time() - start_time\n",
    "    }\n",
    "\n",
    "    # Save metrics to JSON\n",
    "    with open(os.path.join(metrics_dir, f'rnn_attention_model_metrics_{year}.json'), 'w') as f:\n",
    "        json.dump(convert_to_serializable(metrics), f, indent=4)\n",
    "\n",
    "    return metrics, classifier, regressor\n",
    "\n",
    "# Function to train model for a specific year\n",
    "def train_year_rnn_model(year, flight_data_file, output_dir):\n",
    "    \"\"\"\n",
    "    Train a model for a specific year's data using RNN+Attention architecture\n",
    "\n",
    "    Args:\n",
    "        year: Year to train model for\n",
    "        flight_data_file: Path to the flight data file\n",
    "        output_dir: Output directory\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with model results or None if error\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training RNN+Attention model for year {year}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Create year-specific output directories\n",
    "    year_output_dir = os.path.join(output_dir, f'year_{year}_rnn')\n",
    "    os.makedirs(year_output_dir, exist_ok=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 1: Load and preprocess the year's flight data\n",
    "    flight_data = load_and_process_flight_data(flight_data_file)\n",
    "    if flight_data is None or len(flight_data) == 0:\n",
    "        print(f\"No valid flight data available for {year}. Skipping this year.\")\n",
    "        return None\n",
    "\n",
    "    # Step 2: Match weather data\n",
    "    flight_data = match_weather_data(flight_data)\n",
    "\n",
    "    # Step 3: Add advanced feature enhancements\n",
    "    # Add red-eye flight indicator\n",
    "    print(f\"\\nCreating red-eye flight indicator for {year}...\")\n",
    "    flight_data = create_redeye_indicator(flight_data)\n",
    "\n",
    "    # Prepare delay data\n",
    "    print(f\"\\nPreparing delay data for {year}...\")\n",
    "    flight_data = prepare_delay_data(flight_data)\n",
    "\n",
    "    # Create time block features\n",
    "    print(f\"\\nCreating advanced time features for {year}...\")\n",
    "    flight_data = create_advanced_time_features(flight_data)\n",
    "\n",
    "    # Create day features\n",
    "    print(f\"\\nCreating advanced day features for {year}...\")\n",
    "    flight_data = create_advanced_day_features(flight_data)\n",
    "\n",
    "    # Create airport features\n",
    "    print(f\"\\nCreating advanced airport features for {year}...\")\n",
    "    flight_data = create_airport_features(flight_data)\n",
    "\n",
    "    # Create weather features\n",
    "    print(f\"\\nCreating advanced weather features for {year}...\")\n",
    "    flight_data = create_weather_features(flight_data)\n",
    "\n",
    "    # Step 4: Feature selection\n",
    "    print(f\"\\nSelecting features for delay prediction for {year}...\")\n",
    "\n",
    "    # Categorical features\n",
    "    cat_features = [\n",
    "        'TIME_BLOCK', 'DAY_NAME', 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA',\n",
    "        'DISTANCE_CAT'\n",
    "    ]\n",
    "\n",
    "    # Numerical features - include advanced features\n",
    "    num_features = [\n",
    "        # Basic features\n",
    "        'DISTANCE', 'PRCP', 'EXTREME_WEATHER',\n",
    "        'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK',\n",
    "\n",
    "        # Cyclic time encodings\n",
    "        'HOUR_SIN', 'HOUR_COS', 'HALFDAY_SIN', 'HALFDAY_COS',\n",
    "        'QUARTER_DAY_SIN', 'QUARTER_DAY_COS',\n",
    "\n",
    "        # Cyclic day encodings\n",
    "        'DAY_SIN', 'DAY_COS', 'WEEKDAY_SIN', 'WEEKDAY_COS',\n",
    "        'WORKWEEK_SIN', 'WORKWEEK_COS',\n",
    "\n",
    "        # Advanced airport features\n",
    "        'IS_MAJOR_HUB_ORIGIN', 'IS_MAJOR_HUB_DEST', 'IS_HUB_TO_HUB',\n",
    "        'IS_WEST_COAST_ORIGIN', 'IS_EAST_COAST_ORIGIN', 'IS_CENTRAL_ORIGIN',\n",
    "        'IS_WEST_COAST_DEST', 'IS_EAST_COAST_DEST', 'IS_CENTRAL_DEST',\n",
    "        'IS_TRANSCON', 'NORMALIZED_DISTANCE', 'LOG_DISTANCE',\n",
    "\n",
    "        # Advanced weather features\n",
    "        'RAIN_SEVERITY', 'WEATHER_SCORE', 'HUB_WEATHER_IMPACT', 'PEAK_WEATHER_IMPACT'\n",
    "    ]\n",
    "\n",
    "    # Ensure all selected features exist in the dataframe\n",
    "    cat_features = [f for f in cat_features if f in flight_data.columns]\n",
    "    num_features = [f for f in num_features if f in flight_data.columns]\n",
    "\n",
    "    print(f\"Using categorical features: {cat_features}\")\n",
    "    print(f\"Using numerical features: {num_features}\")\n",
    "\n",
    "    # Step 5: Prepare data for modeling\n",
    "    X = flight_data[cat_features + num_features].copy()\n",
    "    y_class = flight_data['IS_DELAYED']\n",
    "\n",
    "    # For regression, use the clipped delay values to avoid extreme outliers\n",
    "    if 'DEP_DELAY_CLIPPED' in flight_data.columns:\n",
    "        y_reg = flight_data['DEP_DELAY_CLIPPED']\n",
    "        print(\"Using clipped delay values for regression to improve neural network training\")\n",
    "    else:\n",
    "        y_reg = flight_data['DEP_DELAY']\n",
    "\n",
    "    # Handle missing values\n",
    "    for col in cat_features:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna('unknown', inplace=True)\n",
    "    for col in num_features:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col].fillna(X[col].median(), inplace=True)\n",
    "\n",
    "    # Preprocess the data - standardize numerical features and one-hot encode categorical features\n",
    "    print(f\"\\nPreprocessing features for neural network training...\")\n",
    "\n",
    "    # Create preprocessing pipeline\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, num_features),\n",
    "            ('cat', categorical_transformer, cat_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Fit and transform the data\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "    print(f\"Processed feature shape: {X_processed.shape}\")\n",
    "\n",
    "    # Split data for training and testing\n",
    "    X_train, X_test, y_train_class, y_test_class = train_test_split(\n",
    "        X_processed, y_class, test_size=0.1, random_state=2025, stratify=y_class\n",
    "    )\n",
    "\n",
    "    # Split data for regression model\n",
    "    _, _, y_train_reg, y_test_reg = train_test_split(\n",
    "        X_processed, y_reg, test_size=0.1, random_state=2025\n",
    "    )\n",
    "\n",
    "    print(f\"Training set size: {X_train.shape}\")\n",
    "    print(f\"Test set size: {X_test.shape}\")\n",
    "\n",
    "    # Step 6: Train RNN+Attention models\n",
    "    metrics, classifier, regressor = train_rnn_attention_models(\n",
    "        X_train, X_test, y_train_class, y_test_class, y_train_reg, y_test_reg,\n",
    "        year, year_output_dir, device\n",
    "    )\n",
    "\n",
    "    # Step 7: Add dataset metrics\n",
    "    metrics.update({\n",
    "        'total_flights': len(flight_data),\n",
    "        'delayed_flights_rate': flight_data['IS_DELAYED'].mean() * 100,\n",
    "        'mean_delay': flight_data['DEP_DELAY'].mean(),\n",
    "        'median_delay': flight_data['DEP_DELAY'].median(),\n",
    "        'max_delay': float(flight_data['DEP_DELAY'].max()),\n",
    "        'min_delay': float(flight_data['DEP_DELAY'].min()),\n",
    "        'feature_count': X_processed.shape[1],\n",
    "        'categorical_features': len(cat_features),\n",
    "        'numerical_features': len(num_features)\n",
    "    })\n",
    "\n",
    "    # Save preprocessor for inference\n",
    "    import joblib\n",
    "    joblib.dump(preprocessor, os.path.join(year_output_dir, f'rnn_preprocessor_{year}.joblib'))\n",
    "    print(f\"Preprocessor saved to {os.path.join(year_output_dir, f'rnn_preprocessor_{year}.joblib')}\")\n",
    "\n",
    "    print(f\"\\nRNN+Attention model training for {year} complete! Total processing time: {time.time() - start_time:.2f} seconds\")\n",
    "    return metrics"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T23:28:20.058212Z",
     "start_time": "2025-04-17T23:16:17.472251Z"
    }
   },
   "source": [
    "def compare_models(all_results, model_type=\"resnet\"):\n",
    "    \"\"\"\n",
    "    Compare models across different years with support for different model types\n",
    "\n",
    "    Args:\n",
    "        all_results: Dictionary with results for each year\n",
    "        model_type: Type of model (e.g., \"resnet\", \"rnn_attention\")\n",
    "\n",
    "    Returns:\n",
    "        None (saves comparison plots)\n",
    "    \"\"\"\n",
    "    print(f\"\\nComparing {model_type.upper()} models across years...\")\n",
    "\n",
    "    if not all_results or len(all_results) < 2:\n",
    "        print(\"Not enough year models to compare.\")\n",
    "        return\n",
    "\n",
    "    # Create a comparison directory\n",
    "    comparison_dir = os.path.join(output_dir, f'comparison_{model_type}')\n",
    "    os.makedirs(comparison_dir, exist_ok=True)\n",
    "\n",
    "    # Extract years and sort them\n",
    "    years = sorted([r['year'] for r in all_results])\n",
    "\n",
    "    # Create DataFrames for different metrics\n",
    "    class_metrics = pd.DataFrame({\n",
    "        'Year': years,\n",
    "        'Accuracy (%)': [r['class_accuracy'] for r in all_results],\n",
    "        'AUC': [r['class_roc_auc'] for r in all_results],\n",
    "        'Precision': [r['class_precision'] for r in all_results],\n",
    "        'Recall': [r['class_recall'] for r in all_results],\n",
    "        'F1 Score': [r['class_f1'] for r in all_results],\n",
    "    })\n",
    "\n",
    "    reg_metrics = pd.DataFrame({\n",
    "        'Year': years,\n",
    "        'RMSE (min)': [r['reg_rmse'] for r in all_results],\n",
    "        'MAE (min)': [r['reg_mae'] for r in all_results],\n",
    "        'R Score': [r['reg_r2'] for r in all_results],\n",
    "    })\n",
    "\n",
    "    timing_metrics = pd.DataFrame({\n",
    "        'Year': years,\n",
    "        'Classification Training Time (s)': [r['class_training_time'] for r in all_results],\n",
    "        'Regression Training Time (s)': [r['reg_training_time'] for r in all_results],\n",
    "        'Classification Epochs': [r.get('class_epochs', 0) for r in all_results],\n",
    "        'Regression Epochs': [r.get('reg_epochs', 0) for r in all_results],\n",
    "    })\n",
    "\n",
    "    delay_stats = pd.DataFrame({\n",
    "        'Year': years,\n",
    "        'Mean Delay (min)': [r['mean_delay'] for r in all_results],\n",
    "        'Delayed Flights (%)': [r['delayed_flights_rate'] for r in all_results],\n",
    "        'Total Flights': [r['total_flights'] for r in all_results],\n",
    "    })\n",
    "\n",
    "    # 1. Plot classification metrics\n",
    "    plt.figure(figsize=(16, 10))\n",
    "\n",
    "    # Set up bar positions\n",
    "    bar_width = 0.15\n",
    "    r1 = np.arange(len(years))\n",
    "    r2 = [x + bar_width for x in r1]\n",
    "    r3 = [x + bar_width for x in r2]\n",
    "    r4 = [x + bar_width for x in r3]\n",
    "    r5 = [x + bar_width for x in r4]\n",
    "\n",
    "    # Create bars\n",
    "    plt.bar(r1, class_metrics['Accuracy (%)'] / 100, width=bar_width, label='Accuracy', color='blue')\n",
    "    plt.bar(r2, class_metrics['AUC'], width=bar_width, label='AUC', color='green')\n",
    "    plt.bar(r3, class_metrics['Precision'], width=bar_width, label='Precision', color='red')\n",
    "    plt.bar(r4, class_metrics['Recall'], width=bar_width, label='Recall', color='purple')\n",
    "    plt.bar(r5, class_metrics['F1 Score'], width=bar_width, label='F1 Score', color='orange')\n",
    "\n",
    "    # Add texts on bars\n",
    "    for i, r in enumerate([r1, r2, r3, r4, r5]):\n",
    "        values = class_metrics.iloc[:, i+1].values\n",
    "        if i == 0:  # Accuracy needs to be divided by 100\n",
    "            values = values / 100\n",
    "        for j, v in enumerate(values):\n",
    "            plt.text(r[j], v + 0.01, f'{v:.2f}' if i > 0 else f'{v*100:.1f}%',\n",
    "                    ha='center', va='bottom', rotation=0, fontsize=8)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title(f'{model_type.upper()} Classification Metrics by Year')\n",
    "    plt.xticks([r + 2*bar_width for r in range(len(years))], years)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1.0)  # Set y-axis limits for better visualization\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_dir, f'{model_type}_classification_metrics_by_year.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Plot regression metrics\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Plot RMSE and MAE\n",
    "    x = np.arange(len(years))\n",
    "    width = 0.35\n",
    "\n",
    "    ax1.bar(x - width/2, reg_metrics['RMSE (min)'], width, label='RMSE')\n",
    "    ax1.bar(x + width/2, reg_metrics['MAE (min)'], width, label='MAE')\n",
    "\n",
    "    # Add text labels\n",
    "    for i, v in enumerate(reg_metrics['RMSE (min)']):\n",
    "        ax1.text(i - width/2, v + 0.5, f'{v:.1f}', ha='center', va='bottom')\n",
    "    for i, v in enumerate(reg_metrics['MAE (min)']):\n",
    "        ax1.text(i + width/2, v + 0.5, f'{v:.1f}', ha='center', va='bottom')\n",
    "\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Minutes')\n",
    "    ax1.set_title(f'{model_type.upper()} Regression Error Metrics')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(years)\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Plot R Score\n",
    "    bars = ax2.bar(years, reg_metrics['R Score'], color='green')\n",
    "\n",
    "    # Add text labels\n",
    "    for bar, value in zip(bars, reg_metrics['R Score']):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, value + 0.01, f'{value:.3f}',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "    ax2.set_xlabel('Year')\n",
    "    ax2.set_ylabel('R Score')\n",
    "    ax2.set_title(f'{model_type.upper()} Regression R Score')\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_dir, f'{model_type}_regression_metrics_by_year.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Plot training times and epochs\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Plot training times\n",
    "    ax1.bar(x - width/2, timing_metrics['Classification Training Time (s)'], width, label='Classification')\n",
    "    ax1.bar(x + width/2, timing_metrics['Regression Training Time (s)'], width, label='Regression')\n",
    "\n",
    "    # Add text labels\n",
    "    for i, v in enumerate(timing_metrics['Classification Training Time (s)']):\n",
    "        ax1.text(i - width/2, v + 5, f'{v:.0f}s', ha='center', va='bottom')\n",
    "    for i, v in enumerate(timing_metrics['Regression Training Time (s)']):\n",
    "        ax1.text(i + width/2, v + 5, f'{v:.0f}s', ha='center', va='bottom')\n",
    "\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Training Time (seconds)')\n",
    "    ax1.set_title(f'{model_type.upper()} Training Times')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(years)\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Plot epochs\n",
    "    ax2.bar(x - width/2, timing_metrics['Classification Epochs'], width, label='Classification')\n",
    "    ax2.bar(x + width/2, timing_metrics['Regression Epochs'], width, label='Regression')\n",
    "\n",
    "    # Add text labels\n",
    "    for i, v in enumerate(timing_metrics['Classification Epochs']):\n",
    "        ax2.text(i - width/2, v + 0.5, f'{v:.0f}', ha='center', va='bottom')\n",
    "    for i, v in enumerate(timing_metrics['Regression Epochs']):\n",
    "        ax2.text(i + width/2, v + 0.5, f'{v:.0f}', ha='center', va='bottom')\n",
    "\n",
    "    ax2.set_xlabel('Year')\n",
    "    ax2.set_ylabel('Number of Epochs')\n",
    "    ax2.set_title(f'{model_type.upper()} Training Epochs')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(years)\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_dir, f'{model_type}_training_metrics_by_year.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 4. Plot delay statistics\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Plot mean delay\n",
    "    bars1 = ax1.bar(years, delay_stats['Mean Delay (min)'], color='blue')\n",
    "\n",
    "    # Add text labels\n",
    "    for bar, value in zip(bars1, delay_stats['Mean Delay (min)']):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, value + 0.3, f'{value:.1f}',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Minutes')\n",
    "    ax1.set_title('Mean Delay by Year')\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Plot delay rate\n",
    "    bars2 = ax2.bar(years, delay_stats['Delayed Flights (%)'], color='red')\n",
    "\n",
    "    # Add text labels\n",
    "    for bar, value in zip(bars2, delay_stats['Delayed Flights (%)']):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, value + 0.5, f'{value:.1f}%',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "    ax2.set_xlabel('Year')\n",
    "    ax2.set_ylabel('Percentage')\n",
    "    ax2.set_title('Delayed Flights Rate by Year')\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(comparison_dir, f'{model_type}_delay_stats_by_year.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # 5. Create a summary table for all metrics\n",
    "    summary = pd.concat([\n",
    "        delay_stats.set_index('Year'),\n",
    "        class_metrics.set_index('Year').iloc[:, 1:],  # Skip the Year column\n",
    "        reg_metrics.set_index('Year').iloc[:, 1:],    # Skip the Year column\n",
    "        timing_metrics.set_index('Year').iloc[:, 1:]  # Skip the Year column\n",
    "    ], axis=1)\n",
    "\n",
    "    # Save the summary to CSV\n",
    "    summary.to_csv(os.path.join(comparison_dir, f'{model_type}_delay_summary.csv'))\n",
    "    print(f\"Comparison summary saved to {os.path.join(comparison_dir, f'{model_type}_delay_summary.csv')}\")\n",
    "\n",
    "    # 6. If we have both ResNet and RNN results, create comparison between model types\n",
    "    resnet_summary_file = os.path.join(output_dir, 'comparison', 'dep_delay_nn_summary.csv')\n",
    "    if model_type == \"rnn_attention\" and os.path.exists(resnet_summary_file):\n",
    "        try:\n",
    "            # Load ResNet summary for comparison\n",
    "            resnet_summary = pd.read_csv(resnet_summary_file)\n",
    "            resnet_summary.set_index('Year', inplace=True)\n",
    "\n",
    "            # Compare classification metrics\n",
    "            compare_metrics(summary, resnet_summary, 'class_accuracy', 'Accuracy (%)', 'Classification Accuracy', comparison_dir)\n",
    "            compare_metrics(summary, resnet_summary, 'class_roc_auc', 'AUC', 'Classification AUC', comparison_dir)\n",
    "\n",
    "            # Compare regression metrics\n",
    "            compare_metrics(summary, resnet_summary, 'reg_rmse', 'RMSE (min)', 'Regression RMSE', comparison_dir)\n",
    "            compare_metrics(summary, resnet_summary, 'reg_r2', 'R Score', 'Regression R Score', comparison_dir)\n",
    "\n",
    "            print(f\"Model comparison (RNN vs ResNet) completed and saved to {comparison_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating model comparison: {e}\")\n",
    "\n",
    "    print(f\"{model_type.upper()} model comparison completed!\")\n",
    "\n",
    "def compare_metrics(rnn_summary, resnet_summary, metric_key, metric_label, title, output_dir):\n",
    "    \"\"\"\n",
    "    Create comparison plots between RNN and ResNet models for a specific metric\n",
    "\n",
    "    Args:\n",
    "        rnn_summary: DataFrame with RNN model metrics\n",
    "        resnet_summary: DataFrame with ResNet model metrics\n",
    "        metric_key: Key of the metric to compare\n",
    "        metric_label: Label for the metric\n",
    "        title: Title for the plot\n",
    "        output_dir: Directory to save the plot\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Ensure we're comparing the same years\n",
    "    common_years = sorted(list(set(rnn_summary.index) & set(resnet_summary.index)))\n",
    "\n",
    "    # Extract data for common years\n",
    "    rnn_data = [rnn_summary.loc[year, metric_key] for year in common_years]\n",
    "    resnet_data = [resnet_summary.loc[year, metric_key] for year in common_years]\n",
    "\n",
    "    # Set up bar positions\n",
    "    x = np.arange(len(common_years))\n",
    "    width = 0.35\n",
    "\n",
    "    # Create bars\n",
    "    bars1 = plt.bar(x - width/2, rnn_data, width, label='RNN+Attention', color='blue')\n",
    "    bars2 = plt.bar(x + width/2, resnet_data, width, label='ResNet', color='red')\n",
    "\n",
    "    # Add text labels\n",
    "    for i, (v1, v2) in enumerate(zip(rnn_data, resnet_data)):\n",
    "        if metric_key == 'class_accuracy':\n",
    "            # Format as percentage\n",
    "            plt.text(i - width/2, v1 + 1, f'{v1:.1f}%', ha='center', va='bottom')\n",
    "            plt.text(i + width/2, v2 + 1, f'{v2:.1f}%', ha='center', va='bottom')\n",
    "        else:\n",
    "            # Format as float\n",
    "            plt.text(i - width/2, v1 + 0.01, f'{v1:.3f}', ha='center', va='bottom')\n",
    "            plt.text(i + width/2, v2 + 0.01, f'{v2:.3f}', ha='center', va='bottom')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel(metric_label)\n",
    "    plt.title(f'Model Comparison: {title}')\n",
    "    plt.xticks(x, common_years)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Save figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'comparison_{metric_key}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Main execution for RNN+Attention models\n",
    "rnn_all_results = []\n",
    "\n",
    "# Process each year's file separately\n",
    "for file_path in flight_files:\n",
    "    year = extract_year_from_filename(file_path)\n",
    "    results = train_year_rnn_model(year, file_path, output_dir)\n",
    "\n",
    "    if results:\n",
    "        rnn_all_results.append(results)\n",
    "        print(f\"\\nRNN+Attention model for year {year} completed successfully!\")\n",
    "    else:\n",
    "        print(f\"\\nRNN+Attention model for year {year} failed.\")\n",
    "\n",
    "# After all individual models are trained, compare them\n",
    "if len(rnn_all_results) > 1:\n",
    "    # You can reuse the comparison function with the RNN results\n",
    "    compare_models(rnn_all_results, model_type=\"rnn_attention\")\n",
    "else:\n",
    "    print(\"\\nNot enough successful RNN+Attention models to perform comparison.\")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\nYear-by-Year RNN+Attention Model Training Summary:\")\n",
    "for year_result in rnn_all_results:\n",
    "    year = year_result['year']\n",
    "    print(f\"\\nYear {year}:\")\n",
    "    print(f\"  Total flights: {year_result['total_flights']:,}\")\n",
    "    print(f\"  Classification accuracy: {year_result['class_accuracy']:.2f}%\")\n",
    "    print(f\"  Classification AUC: {year_result['class_roc_auc']:.4f}\")\n",
    "    print(f\"  Regression RMSE: {year_result['reg_rmse']:.2f} minutes\")\n",
    "    print(f\"  Regression R: {year_result['reg_r2']:.4f}\")\n",
    "    print(f\"  Mean delay: {year_result['mean_delay']:.2f} minutes\")\n",
    "    print(f\"  Delay rate: {year_result['delayed_flights_rate']:.2f}%\")\n",
    "    print(f\"  Training time: {year_result['total_processing_time']:.2f} seconds\")\n",
    "\n",
    "print(\"\\nRNN+Attention model training complete! Check output directories for detailed results.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Training RNN+Attention model for year 2021\n",
      "================================================================================\n",
      "\n",
      "Processing May2021.csv...\n",
      "Years found in data: [2021]\n",
      "Months found in data: {5: 520059}\n",
      "Filtered to only May data: 520059 rows\n",
      "Filtered from 520059 to 171867 rows for top 30 airports\n",
      "Removed 485.0 cancelled flights, remaining: 171382\n",
      "Processing took: 2.77 seconds\n",
      "\n",
      "Matching weather data with flights...\n",
      "Processed 10000/171382 rows, matched 7633 flights with weather data\n",
      "Processed 20000/171382 rows, matched 15255 flights with weather data\n",
      "Processed 30000/171382 rows, matched 22811 flights with weather data\n",
      "Processed 40000/171382 rows, matched 30386 flights with weather data\n",
      "Processed 50000/171382 rows, matched 37926 flights with weather data\n",
      "Processed 60000/171382 rows, matched 45609 flights with weather data\n",
      "Processed 70000/171382 rows, matched 53275 flights with weather data\n",
      "Processed 80000/171382 rows, matched 60864 flights with weather data\n",
      "Processed 90000/171382 rows, matched 68367 flights with weather data\n",
      "Processed 100000/171382 rows, matched 75976 flights with weather data\n",
      "Processed 110000/171382 rows, matched 83600 flights with weather data\n",
      "Processed 120000/171382 rows, matched 91287 flights with weather data\n",
      "Processed 130000/171382 rows, matched 98785 flights with weather data\n",
      "Processed 140000/171382 rows, matched 106365 flights with weather data\n",
      "Processed 150000/171382 rows, matched 113975 flights with weather data\n",
      "Processed 160000/171382 rows, matched 121693 flights with weather data\n",
      "Processed 170000/171382 rows, matched 129253 flights with weather data\n",
      "Processed 171382/171382 rows, matched 130389 flights with weather data\n",
      "Matched weather data for 130389 flights (76.08%)\n",
      "Weather matching took: 60.36 seconds\n",
      "\n",
      "Creating red-eye flight indicator for 2021...\n",
      "Identified 2854 red-eye flights based on departure time (0-6 AM)\n",
      "Identified 4648 red-eye flights based on arrival time (0-6 AM)\n",
      "Total identified red-eye flights: 7186 out of 171382 total flights (4.19%)\n",
      "\n",
      "Distribution of flights by departure time of day:\n",
      "  - Morning (6-12): 67291 flights (39.26%)\n",
      "  - Afternoon (12-18): 61243 flights (35.73%)\n",
      "  - Evening (18-24): 35957 flights (20.98%)\n",
      "  - Early Morning (0-6): 6891 flights (4.02%)\n",
      "\n",
      "Preparing delay data for 2021...\n",
      "\n",
      "Delay statistics:\n",
      "Delayed flights: 54865/171382 (32.01%)\n",
      "On-time or early flights: 116517/171382 (67.99%)\n",
      "\n",
      "Delay magnitude statistics:\n",
      "Mean delay: 6.99 minutes\n",
      "Median delay: -2.00 minutes\n",
      "Min delay: -35.00 minutes (negative means early departure)\n",
      "Max delay: 1800.00 minutes\n",
      "Clipped delay values above 223.00 minutes for neural network training\n",
      "Number of clipped values: 856\n",
      "\n",
      "Delay category distribution:\n",
      "  - Very Early: 355 flights (0.21%)\n",
      "  - Early: 116162 flights (67.78%)\n",
      "  - On Time: 31919 flights (18.62%)\n",
      "  - Moderate Delay: 15864 flights (9.26%)\n",
      "  - Significant Delay: 4268 flights (2.49%)\n",
      "  - Severe Delay: 2814 flights (1.64%)\n",
      "\n",
      "Creating advanced time features for 2021...\n",
      "\n",
      "Creating advanced day features for 2021...\n",
      "\n",
      "Distribution of flights by day of week:\n",
      "  - Monday: 28849 flights (16.83%)\n",
      "  - Sunday: 28404 flights (16.57%)\n",
      "  - Saturday: 25897 flights (15.11%)\n",
      "  - Friday: 23495 flights (13.71%)\n",
      "  - Thursday: 23320 flights (13.61%)\n",
      "  - Wednesday: 21034 flights (12.27%)\n",
      "  - Tuesday: 20383 flights (11.89%)\n",
      "\n",
      "Weekend flights: 54301 (31.68%)\n",
      "Weekday flights: 117081 (68.32%)\n",
      "\n",
      "Creating advanced airport features for 2021...\n",
      "\n",
      "Creating advanced weather features for 2021...\n",
      "\n",
      "Selecting features for delay prediction for 2021...\n",
      "Using categorical features: ['TIME_BLOCK', 'DAY_NAME', 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'DISTANCE_CAT']\n",
      "Using numerical features: ['DISTANCE', 'PRCP', 'EXTREME_WEATHER', 'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK', 'HOUR_SIN', 'HOUR_COS', 'HALFDAY_SIN', 'HALFDAY_COS', 'QUARTER_DAY_SIN', 'QUARTER_DAY_COS', 'DAY_SIN', 'DAY_COS', 'WEEKDAY_SIN', 'WEEKDAY_COS', 'WORKWEEK_SIN', 'WORKWEEK_COS', 'IS_MAJOR_HUB_ORIGIN', 'IS_MAJOR_HUB_DEST', 'IS_HUB_TO_HUB', 'IS_WEST_COAST_ORIGIN', 'IS_EAST_COAST_ORIGIN', 'IS_CENTRAL_ORIGIN', 'IS_WEST_COAST_DEST', 'IS_EAST_COAST_DEST', 'IS_CENTRAL_DEST', 'IS_TRANSCON', 'NORMALIZED_DISTANCE', 'LOG_DISTANCE', 'RAIN_SEVERITY', 'WEATHER_SCORE', 'HUB_WEATHER_IMPACT', 'PEAK_WEATHER_IMPACT']\n",
      "Using clipped delay values for regression to improve neural network training\n",
      "\n",
      "Preprocessing features for neural network training...\n",
      "Processed feature shape: (171382, 124)\n",
      "Training set size: (154243, 124)\n",
      "Test set size: (17139, 124)\n",
      "\n",
      "Building and training RNN+Attention neural network models...\n",
      "\n",
      "Training RNN+Attention classification model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|| 151/151 [00:01<00:00, 78.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.5936, Val Loss: 0.5717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|| 151/151 [00:01<00:00, 85.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 0.5708, Val Loss: 0.5676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|| 151/151 [00:01<00:00, 85.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 0.5652, Val Loss: 0.5653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|| 151/151 [00:01<00:00, 88.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 0.5616, Val Loss: 0.5640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|| 151/151 [00:01<00:00, 83.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 0.5586, Val Loss: 0.5623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|| 151/151 [00:01<00:00, 91.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 0.5563, Val Loss: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|| 151/151 [00:01<00:00, 82.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 0.5539, Val Loss: 0.5596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|| 151/151 [00:01<00:00, 87.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 0.5521, Val Loss: 0.5596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|| 151/151 [00:01<00:00, 85.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 0.5500, Val Loss: 0.5592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|| 151/151 [00:01<00:00, 84.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 0.5483, Val Loss: 0.5584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|| 151/151 [00:01<00:00, 81.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 0.5468, Val Loss: 0.5570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|| 151/151 [00:01<00:00, 85.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 0.5463, Val Loss: 0.5572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|| 151/151 [00:01<00:00, 84.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Loss: 0.5440, Val Loss: 0.5579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|| 151/151 [00:01<00:00, 81.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Loss: 0.5426, Val Loss: 0.5585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|| 151/151 [00:01<00:00, 80.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Loss: 0.5413, Val Loss: 0.5566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|| 151/151 [00:01<00:00, 84.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Train Loss: 0.5399, Val Loss: 0.5562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|| 151/151 [00:01<00:00, 87.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Loss: 0.5391, Val Loss: 0.5578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|| 151/151 [00:01<00:00, 82.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Loss: 0.5383, Val Loss: 0.5573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|| 151/151 [00:01<00:00, 82.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Loss: 0.5371, Val Loss: 0.5568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|| 151/151 [00:01<00:00, 88.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Train Loss: 0.5360, Val Loss: 0.5578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|| 151/151 [00:01<00:00, 78.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Loss: 0.5309, Val Loss: 0.5561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|| 151/151 [00:01<00:00, 82.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Train Loss: 0.5288, Val Loss: 0.5573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|| 151/151 [00:01<00:00, 84.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Train Loss: 0.5276, Val Loss: 0.5570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|| 151/151 [00:01<00:00, 85.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Loss: 0.5270, Val Loss: 0.5561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|| 151/151 [00:01<00:00, 84.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Train Loss: 0.5234, Val Loss: 0.5566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|| 151/151 [00:01<00:00, 87.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Loss: 0.5220, Val Loss: 0.5570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|| 151/151 [00:01<00:00, 85.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Train Loss: 0.5211, Val Loss: 0.5569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|| 151/151 [00:01<00:00, 83.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Loss: 0.5204, Val Loss: 0.5573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|| 151/151 [00:01<00:00, 81.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Loss: 0.5179, Val Loss: 0.5577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|| 151/151 [00:01<00:00, 81.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Loss: 0.5183, Val Loss: 0.5579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|| 151/151 [00:01<00:00, 84.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Train Loss: 0.5177, Val Loss: 0.5579\n",
      "Early stopping triggered after 31 epochs\n",
      "Classification model training took: 60.55 seconds\n",
      "\n",
      "Training RNN+Attention regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|| 151/151 [00:01<00:00, 86.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 117.6980, Val Loss: 115.5055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|| 151/151 [00:01<00:00, 79.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 117.6468, Val Loss: 115.5032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|| 151/151 [00:01<00:00, 76.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 117.6329, Val Loss: 115.5083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|| 151/151 [00:01<00:00, 84.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 117.6249, Val Loss: 115.5056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|| 151/151 [00:01<00:00, 89.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 117.6096, Val Loss: 115.5142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|| 151/151 [00:01<00:00, 81.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 117.5850, Val Loss: 115.5563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|| 151/151 [00:01<00:00, 79.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 117.5584, Val Loss: 115.5650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|| 151/151 [00:01<00:00, 80.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 117.5415, Val Loss: 115.6051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|| 151/151 [00:01<00:00, 77.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 117.5248, Val Loss: 115.6344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|| 151/151 [00:01<00:00, 81.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 117.4792, Val Loss: 115.6512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|| 151/151 [00:01<00:00, 80.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 117.4706, Val Loss: 115.6685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|| 151/151 [00:01<00:00, 83.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 117.4373, Val Loss: 115.6902\n",
      "Early stopping triggered after 12 epochs\n",
      "Regression model training took: 23.91 seconds\n",
      "\n",
      "Evaluating classification model...\n",
      "Classification Accuracy: 72.61%\n",
      "Classification ROC AUC: 0.7241\n",
      "Warning: Class '1' not present in test results. Using default metrics.\n",
      "\n",
      "Evaluating regression model...\n",
      "Regression Mean Squared Error: 849.15\n",
      "Regression Root Mean Squared Error: 29.14 minutes\n",
      "Regression Mean Absolute Error: 12.02 minutes\n",
      "Regression R Score: -0.0345\n",
      "Preprocessor saved to ./dep_delay_nn/year_2021_rnn\\rnn_preprocessor_2021.joblib\n",
      "\n",
      "RNN+Attention model training for 2021 complete! Total processing time: 150.41 seconds\n",
      "\n",
      "RNN+Attention model for year 2021 completed successfully!\n",
      "\n",
      "================================================================================\n",
      "Training RNN+Attention model for year 2022\n",
      "================================================================================\n",
      "\n",
      "Processing May2022.csv...\n",
      "Years found in data: [2022]\n",
      "Months found in data: {5: 602950}\n",
      "Filtered to only May data: 602950 rows\n",
      "Filtered from 602950 to 210079 rows for top 30 airports\n",
      "Removed 4659.0 cancelled flights, remaining: 205420\n",
      "Processing took: 3.09 seconds\n",
      "\n",
      "Matching weather data with flights...\n",
      "Processed 10000/205420 rows, matched 7065 flights with weather data\n",
      "Processed 20000/205420 rows, matched 14381 flights with weather data\n",
      "Processed 30000/205420 rows, matched 21409 flights with weather data\n",
      "Processed 40000/205420 rows, matched 28736 flights with weather data\n",
      "Processed 50000/205420 rows, matched 35905 flights with weather data\n",
      "Processed 60000/205420 rows, matched 43280 flights with weather data\n",
      "Processed 70000/205420 rows, matched 50286 flights with weather data\n",
      "Processed 80000/205420 rows, matched 57605 flights with weather data\n",
      "Processed 90000/205420 rows, matched 64757 flights with weather data\n",
      "Processed 100000/205420 rows, matched 72193 flights with weather data\n",
      "Processed 110000/205420 rows, matched 79374 flights with weather data\n",
      "Processed 120000/205420 rows, matched 86687 flights with weather data\n",
      "Processed 130000/205420 rows, matched 93773 flights with weather data\n",
      "Processed 140000/205420 rows, matched 101221 flights with weather data\n",
      "Processed 150000/205420 rows, matched 108364 flights with weather data\n",
      "Processed 160000/205420 rows, matched 115722 flights with weather data\n",
      "Processed 170000/205420 rows, matched 122731 flights with weather data\n",
      "Processed 180000/205420 rows, matched 130269 flights with weather data\n",
      "Processed 190000/205420 rows, matched 137386 flights with weather data\n",
      "Processed 200000/205420 rows, matched 144660 flights with weather data\n",
      "Processed 205420/205420 rows, matched 148501 flights with weather data\n",
      "Matched weather data for 148501 flights (72.29%)\n",
      "Weather matching took: 67.49 seconds\n",
      "\n",
      "Creating red-eye flight indicator for 2022...\n",
      "Identified 6093 red-eye flights based on departure time (0-6 AM)\n",
      "Identified 7457 red-eye flights based on arrival time (0-6 AM)\n",
      "Total identified red-eye flights: 13088 out of 205420 total flights (6.37%)\n",
      "\n",
      "Distribution of flights by departure time of day:\n",
      "  - Morning (6-12): 78343 flights (38.14%)\n",
      "  - Afternoon (12-18): 69688 flights (33.92%)\n",
      "  - Evening (18-24): 46622 flights (22.70%)\n",
      "  - Early Morning (0-6): 10767 flights (5.24%)\n",
      "\n",
      "Preparing delay data for 2022...\n",
      "\n",
      "Delay statistics:\n",
      "Delayed flights: 91269/205420 (44.43%)\n",
      "On-time or early flights: 114151/205420 (55.57%)\n",
      "\n",
      "Delay magnitude statistics:\n",
      "Mean delay: 14.56 minutes\n",
      "Median delay: 0.00 minutes\n",
      "Min delay: -34.00 minutes (negative means early departure)\n",
      "Max delay: 2109.00 minutes\n",
      "Clipped delay values above 280.00 minutes for neural network training\n",
      "Number of clipped values: 1023\n",
      "\n",
      "Delay category distribution:\n",
      "  - Very Early: 208 flights (0.10%)\n",
      "  - Early: 113943 flights (55.47%)\n",
      "  - On Time: 44584 flights (21.70%)\n",
      "  - Moderate Delay: 31050 flights (15.12%)\n",
      "  - Significant Delay: 9754 flights (4.75%)\n",
      "  - Severe Delay: 5881 flights (2.86%)\n",
      "\n",
      "Creating advanced time features for 2022...\n",
      "\n",
      "Creating advanced day features for 2022...\n",
      "\n",
      "Distribution of flights by day of week:\n",
      "  - Monday: 34057 flights (16.58%)\n",
      "  - Tuesday: 33242 flights (16.18%)\n",
      "  - Sunday: 33153 flights (16.14%)\n",
      "  - Thursday: 28003 flights (13.63%)\n",
      "  - Friday: 26923 flights (13.11%)\n",
      "  - Wednesday: 26859 flights (13.08%)\n",
      "  - Saturday: 23183 flights (11.29%)\n",
      "\n",
      "Weekend flights: 56336 (27.42%)\n",
      "Weekday flights: 149084 (72.58%)\n",
      "\n",
      "Creating advanced airport features for 2022...\n",
      "\n",
      "Creating advanced weather features for 2022...\n",
      "\n",
      "Selecting features for delay prediction for 2022...\n",
      "Using categorical features: ['TIME_BLOCK', 'DAY_NAME', 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'DISTANCE_CAT']\n",
      "Using numerical features: ['DISTANCE', 'PRCP', 'EXTREME_WEATHER', 'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK', 'HOUR_SIN', 'HOUR_COS', 'HALFDAY_SIN', 'HALFDAY_COS', 'QUARTER_DAY_SIN', 'QUARTER_DAY_COS', 'DAY_SIN', 'DAY_COS', 'WEEKDAY_SIN', 'WEEKDAY_COS', 'WORKWEEK_SIN', 'WORKWEEK_COS', 'IS_MAJOR_HUB_ORIGIN', 'IS_MAJOR_HUB_DEST', 'IS_HUB_TO_HUB', 'IS_WEST_COAST_ORIGIN', 'IS_EAST_COAST_ORIGIN', 'IS_CENTRAL_ORIGIN', 'IS_WEST_COAST_DEST', 'IS_EAST_COAST_DEST', 'IS_CENTRAL_DEST', 'IS_TRANSCON', 'NORMALIZED_DISTANCE', 'LOG_DISTANCE', 'RAIN_SEVERITY', 'WEATHER_SCORE', 'HUB_WEATHER_IMPACT', 'PEAK_WEATHER_IMPACT']\n",
      "Using clipped delay values for regression to improve neural network training\n",
      "\n",
      "Preprocessing features for neural network training...\n",
      "Processed feature shape: (205420, 124)\n",
      "Training set size: (184878, 124)\n",
      "Test set size: (20542, 124)\n",
      "\n",
      "Building and training RNN+Attention neural network models...\n",
      "\n",
      "Training RNN+Attention classification model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|| 181/181 [00:02<00:00, 78.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.6382, Val Loss: 0.6230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|| 181/181 [00:02<00:00, 82.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 0.6241, Val Loss: 0.6202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|| 181/181 [00:02<00:00, 84.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 0.6202, Val Loss: 0.6192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|| 181/181 [00:02<00:00, 82.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 0.6180, Val Loss: 0.6169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|| 181/181 [00:02<00:00, 83.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 0.6160, Val Loss: 0.6154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|| 181/181 [00:02<00:00, 80.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 0.6133, Val Loss: 0.6144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|| 181/181 [00:02<00:00, 85.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 0.6119, Val Loss: 0.6141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|| 181/181 [00:02<00:00, 79.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 0.6103, Val Loss: 0.6140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|| 181/181 [00:02<00:00, 82.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 0.6088, Val Loss: 0.6144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|| 181/181 [00:02<00:00, 81.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 0.6078, Val Loss: 0.6130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|| 181/181 [00:02<00:00, 84.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 0.6064, Val Loss: 0.6127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|| 181/181 [00:02<00:00, 85.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 0.6047, Val Loss: 0.6134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|| 181/181 [00:02<00:00, 77.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Loss: 0.6040, Val Loss: 0.6117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|| 181/181 [00:02<00:00, 83.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Loss: 0.6027, Val Loss: 0.6121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|| 181/181 [00:02<00:00, 81.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Loss: 0.6017, Val Loss: 0.6107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|| 181/181 [00:02<00:00, 82.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Train Loss: 0.6008, Val Loss: 0.6103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|| 181/181 [00:02<00:00, 85.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Loss: 0.6001, Val Loss: 0.6113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|| 181/181 [00:02<00:00, 83.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Loss: 0.5991, Val Loss: 0.6117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|| 181/181 [00:02<00:00, 83.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Loss: 0.5980, Val Loss: 0.6126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|| 181/181 [00:02<00:00, 83.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Train Loss: 0.5972, Val Loss: 0.6099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|| 181/181 [00:02<00:00, 84.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Loss: 0.5966, Val Loss: 0.6111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|| 181/181 [00:02<00:00, 79.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Train Loss: 0.5957, Val Loss: 0.6094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|| 181/181 [00:02<00:00, 83.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Train Loss: 0.5944, Val Loss: 0.6090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|| 181/181 [00:02<00:00, 83.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Loss: 0.5933, Val Loss: 0.6108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|| 181/181 [00:02<00:00, 83.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Train Loss: 0.5929, Val Loss: 0.6102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|| 181/181 [00:02<00:00, 83.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Loss: 0.5915, Val Loss: 0.6103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|| 181/181 [00:02<00:00, 80.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Train Loss: 0.5909, Val Loss: 0.6099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|| 181/181 [00:02<00:00, 83.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Loss: 0.5869, Val Loss: 0.6104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|| 181/181 [00:02<00:00, 81.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Loss: 0.5847, Val Loss: 0.6104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|| 181/181 [00:02<00:00, 82.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Loss: 0.5844, Val Loss: 0.6094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|| 181/181 [00:02<00:00, 86.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Train Loss: 0.5835, Val Loss: 0.6103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|| 181/181 [00:02<00:00, 83.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Train Loss: 0.5799, Val Loss: 0.6103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|| 181/181 [00:02<00:00, 83.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Train Loss: 0.5791, Val Loss: 0.6100\n",
      "Early stopping triggered after 33 epochs\n",
      "Classification model training took: 77.59 seconds\n",
      "\n",
      "Training RNN+Attention regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|| 181/181 [00:02<00:00, 83.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 198.3098, Val Loss: 197.3488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|| 181/181 [00:02<00:00, 83.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 197.8185, Val Loss: 197.2995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|| 181/181 [00:02<00:00, 81.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 197.7668, Val Loss: 197.3083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|| 181/181 [00:02<00:00, 79.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 197.7687, Val Loss: 197.3158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|| 181/181 [00:02<00:00, 83.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 197.7393, Val Loss: 197.3236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|| 181/181 [00:02<00:00, 85.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 197.7209, Val Loss: 197.3637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|| 181/181 [00:02<00:00, 79.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 197.6708, Val Loss: 197.3452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|| 181/181 [00:02<00:00, 77.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 197.6257, Val Loss: 197.4048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|| 181/181 [00:02<00:00, 79.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 197.6098, Val Loss: 197.3930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|| 181/181 [00:02<00:00, 83.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 197.5695, Val Loss: 197.4370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|| 181/181 [00:02<00:00, 81.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 197.5317, Val Loss: 197.4763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|| 181/181 [00:02<00:00, 82.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 197.5023, Val Loss: 197.4759\n",
      "Early stopping triggered after 12 epochs\n",
      "Regression model training took: 28.64 seconds\n",
      "\n",
      "Evaluating classification model...\n",
      "Classification Accuracy: 66.33%\n",
      "Classification ROC AUC: 0.7188\n",
      "Warning: Class '1' not present in test results. Using default metrics.\n",
      "\n",
      "Evaluating regression model...\n",
      "Regression Mean Squared Error: 1576.67\n",
      "Regression Root Mean Squared Error: 39.71 minutes\n",
      "Regression Mean Absolute Error: 18.52 minutes\n",
      "Regression R Score: -0.0657\n",
      "Preprocessor saved to ./dep_delay_nn/year_2022_rnn\\rnn_preprocessor_2022.joblib\n",
      "\n",
      "RNN+Attention model training for 2022 complete! Total processing time: 179.85 seconds\n",
      "\n",
      "RNN+Attention model for year 2022 completed successfully!\n",
      "\n",
      "================================================================================\n",
      "Training RNN+Attention model for year 2023\n",
      "================================================================================\n",
      "\n",
      "Processing May2023.csv...\n",
      "Years found in data: [2023]\n",
      "Months found in data: {5: 616630}\n",
      "Filtered to only May data: 616630 rows\n",
      "Filtered from 616630 to 220469 rows for top 30 airports\n",
      "Removed 1293.0 cancelled flights, remaining: 219176\n",
      "Processing took: 3.18 seconds\n",
      "\n",
      "Matching weather data with flights...\n",
      "Processed 10000/219176 rows, matched 7071 flights with weather data\n",
      "Processed 20000/219176 rows, matched 14364 flights with weather data\n",
      "Processed 30000/219176 rows, matched 21626 flights with weather data\n",
      "Processed 40000/219176 rows, matched 28902 flights with weather data\n",
      "Processed 50000/219176 rows, matched 36373 flights with weather data\n",
      "Processed 60000/219176 rows, matched 43539 flights with weather data\n",
      "Processed 70000/219176 rows, matched 50884 flights with weather data\n",
      "Processed 80000/219176 rows, matched 58144 flights with weather data\n",
      "Processed 90000/219176 rows, matched 65463 flights with weather data\n",
      "Processed 100000/219176 rows, matched 72856 flights with weather data\n",
      "Processed 110000/219176 rows, matched 79891 flights with weather data\n",
      "Processed 120000/219176 rows, matched 86951 flights with weather data\n",
      "Processed 130000/219176 rows, matched 93797 flights with weather data\n",
      "Processed 140000/219176 rows, matched 101252 flights with weather data\n",
      "Processed 150000/219176 rows, matched 108560 flights with weather data\n",
      "Processed 160000/219176 rows, matched 115748 flights with weather data\n",
      "Processed 170000/219176 rows, matched 123276 flights with weather data\n",
      "Processed 180000/219176 rows, matched 130383 flights with weather data\n",
      "Processed 190000/219176 rows, matched 137913 flights with weather data\n",
      "Processed 200000/219176 rows, matched 145101 flights with weather data\n",
      "Processed 210000/219176 rows, matched 152449 flights with weather data\n",
      "Processed 219176/219176 rows, matched 159344 flights with weather data\n",
      "Matched weather data for 159344 flights (72.70%)\n",
      "Weather matching took: 72.76 seconds\n",
      "\n",
      "Creating red-eye flight indicator for 2023...\n",
      "Identified 6485 red-eye flights based on departure time (0-6 AM)\n",
      "Identified 8863 red-eye flights based on arrival time (0-6 AM)\n",
      "Total identified red-eye flights: 15095 out of 219176 total flights (6.89%)\n",
      "\n",
      "Distribution of flights by departure time of day:\n",
      "  - Morning (6-12): 82862 flights (37.81%)\n",
      "  - Afternoon (12-18): 73196 flights (33.40%)\n",
      "  - Evening (18-24): 51478 flights (23.49%)\n",
      "  - Early Morning (0-6): 11640 flights (5.31%)\n",
      "\n",
      "Preparing delay data for 2023...\n",
      "\n",
      "Delay statistics:\n",
      "Delayed flights: 87833/219176 (40.07%)\n",
      "On-time or early flights: 131343/219176 (59.93%)\n",
      "\n",
      "Delay magnitude statistics:\n",
      "Mean delay: 12.35 minutes\n",
      "Median delay: -1.00 minutes\n",
      "Min delay: -29.00 minutes (negative means early departure)\n",
      "Max delay: 3221.00 minutes\n",
      "Clipped delay values above 278.00 minutes for neural network training\n",
      "Number of clipped values: 1093\n",
      "\n",
      "Delay category distribution:\n",
      "  - Very Early: 428 flights (0.20%)\n",
      "  - Early: 130915 flights (59.73%)\n",
      "  - On Time: 44120 flights (20.13%)\n",
      "  - Moderate Delay: 29279 flights (13.36%)\n",
      "  - Significant Delay: 8888 flights (4.06%)\n",
      "  - Severe Delay: 5546 flights (2.53%)\n",
      "\n",
      "Creating advanced time features for 2023...\n",
      "\n",
      "Creating advanced day features for 2023...\n",
      "\n",
      "Distribution of flights by day of week:\n",
      "  - Monday: 36506 flights (16.66%)\n",
      "  - Wednesday: 35633 flights (16.26%)\n",
      "  - Tuesday: 35382 flights (16.14%)\n",
      "  - Thursday: 29268 flights (13.35%)\n",
      "  - Friday: 29138 flights (13.29%)\n",
      "  - Sunday: 28009 flights (12.78%)\n",
      "  - Saturday: 25240 flights (11.52%)\n",
      "\n",
      "Weekend flights: 53249 (24.30%)\n",
      "Weekday flights: 165927 (75.70%)\n",
      "\n",
      "Creating advanced airport features for 2023...\n",
      "\n",
      "Creating advanced weather features for 2023...\n",
      "\n",
      "Selecting features for delay prediction for 2023...\n",
      "Using categorical features: ['TIME_BLOCK', 'DAY_NAME', 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'DISTANCE_CAT']\n",
      "Using numerical features: ['DISTANCE', 'PRCP', 'EXTREME_WEATHER', 'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK', 'HOUR_SIN', 'HOUR_COS', 'HALFDAY_SIN', 'HALFDAY_COS', 'QUARTER_DAY_SIN', 'QUARTER_DAY_COS', 'DAY_SIN', 'DAY_COS', 'WEEKDAY_SIN', 'WEEKDAY_COS', 'WORKWEEK_SIN', 'WORKWEEK_COS', 'IS_MAJOR_HUB_ORIGIN', 'IS_MAJOR_HUB_DEST', 'IS_HUB_TO_HUB', 'IS_WEST_COAST_ORIGIN', 'IS_EAST_COAST_ORIGIN', 'IS_CENTRAL_ORIGIN', 'IS_WEST_COAST_DEST', 'IS_EAST_COAST_DEST', 'IS_CENTRAL_DEST', 'IS_TRANSCON', 'NORMALIZED_DISTANCE', 'LOG_DISTANCE', 'RAIN_SEVERITY', 'WEATHER_SCORE', 'HUB_WEATHER_IMPACT', 'PEAK_WEATHER_IMPACT']\n",
      "Using clipped delay values for regression to improve neural network training\n",
      "\n",
      "Preprocessing features for neural network training...\n",
      "Processed feature shape: (219176, 124)\n",
      "Training set size: (197258, 124)\n",
      "Test set size: (21918, 124)\n",
      "\n",
      "Building and training RNN+Attention neural network models...\n",
      "\n",
      "Training RNN+Attention classification model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|| 193/193 [00:02<00:00, 77.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.6221, Val Loss: 0.6009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|| 193/193 [00:02<00:00, 86.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 0.6082, Val Loss: 0.5997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|| 193/193 [00:02<00:00, 84.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 0.6041, Val Loss: 0.5964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|| 193/193 [00:02<00:00, 81.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 0.6014, Val Loss: 0.5951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|| 193/193 [00:02<00:00, 82.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 0.5992, Val Loss: 0.5937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|| 193/193 [00:02<00:00, 81.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 0.5973, Val Loss: 0.5928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|| 193/193 [00:02<00:00, 81.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 0.5956, Val Loss: 0.5915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|| 193/193 [00:02<00:00, 84.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 0.5943, Val Loss: 0.5905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|| 193/193 [00:02<00:00, 81.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 0.5923, Val Loss: 0.5892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|| 193/193 [00:02<00:00, 84.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 0.5907, Val Loss: 0.5887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|| 193/193 [00:02<00:00, 83.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 0.5891, Val Loss: 0.5875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|| 193/193 [00:02<00:00, 83.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 0.5882, Val Loss: 0.5889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|| 193/193 [00:02<00:00, 83.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Loss: 0.5872, Val Loss: 0.5874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|| 193/193 [00:02<00:00, 83.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Loss: 0.5861, Val Loss: 0.5874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|| 193/193 [00:02<00:00, 82.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Loss: 0.5849, Val Loss: 0.5866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|| 193/193 [00:02<00:00, 84.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Train Loss: 0.5839, Val Loss: 0.5870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|| 193/193 [00:02<00:00, 84.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Loss: 0.5825, Val Loss: 0.5863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|| 193/193 [00:02<00:00, 84.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Loss: 0.5820, Val Loss: 0.5852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|| 193/193 [00:02<00:00, 84.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Loss: 0.5812, Val Loss: 0.5849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|| 193/193 [00:02<00:00, 82.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Train Loss: 0.5796, Val Loss: 0.5855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|| 193/193 [00:02<00:00, 82.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Loss: 0.5785, Val Loss: 0.5865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|| 193/193 [00:02<00:00, 83.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Train Loss: 0.5783, Val Loss: 0.5857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|| 193/193 [00:02<00:00, 84.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Train Loss: 0.5776, Val Loss: 0.5862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|| 193/193 [00:02<00:00, 82.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Loss: 0.5725, Val Loss: 0.5844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|| 193/193 [00:02<00:00, 86.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Train Loss: 0.5711, Val Loss: 0.5848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|| 193/193 [00:02<00:00, 82.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Loss: 0.5703, Val Loss: 0.5849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|| 193/193 [00:02<00:00, 82.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Train Loss: 0.5701, Val Loss: 0.5851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|| 193/193 [00:02<00:00, 86.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Loss: 0.5692, Val Loss: 0.5852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|| 193/193 [00:02<00:00, 82.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Loss: 0.5657, Val Loss: 0.5857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|| 193/193 [00:02<00:00, 83.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Loss: 0.5656, Val Loss: 0.5846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|| 193/193 [00:02<00:00, 82.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Train Loss: 0.5651, Val Loss: 0.5849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|| 193/193 [00:02<00:00, 83.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Train Loss: 0.5638, Val Loss: 0.5857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|| 193/193 [00:02<00:00, 83.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Train Loss: 0.5628, Val Loss: 0.5853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|| 193/193 [00:02<00:00, 82.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Train Loss: 0.5620, Val Loss: 0.5859\n",
      "Early stopping triggered after 34 epochs\n",
      "Classification model training took: 85.09 seconds\n",
      "\n",
      "Training RNN+Attention regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|| 193/193 [00:02<00:00, 85.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 178.4502, Val Loss: 181.5029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|| 193/193 [00:02<00:00, 84.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 178.2558, Val Loss: 181.4534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|| 193/193 [00:02<00:00, 80.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 178.2152, Val Loss: 181.4742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|| 193/193 [00:02<00:00, 82.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 178.1975, Val Loss: 181.4658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|| 193/193 [00:02<00:00, 79.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 178.1875, Val Loss: 181.4559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|| 193/193 [00:02<00:00, 78.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 178.1477, Val Loss: 181.4764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|| 193/193 [00:02<00:00, 84.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 178.0923, Val Loss: 181.4957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|| 193/193 [00:02<00:00, 83.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 178.0669, Val Loss: 181.5065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|| 193/193 [00:02<00:00, 81.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 178.0402, Val Loss: 181.5756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|| 193/193 [00:02<00:00, 81.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 177.9989, Val Loss: 181.5468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|| 193/193 [00:02<00:00, 82.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 177.9372, Val Loss: 181.6159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|| 193/193 [00:02<00:00, 82.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 177.8967, Val Loss: 181.6143\n",
      "Early stopping triggered after 12 epochs\n",
      "Regression model training took: 30.48 seconds\n",
      "\n",
      "Evaluating classification model...\n",
      "Classification Accuracy: 69.25%\n",
      "Classification ROC AUC: 0.7355\n",
      "Warning: Class '1' not present in test results. Using default metrics.\n",
      "\n",
      "Evaluating regression model...\n",
      "Regression Mean Squared Error: 1464.75\n",
      "Regression Root Mean Squared Error: 38.27 minutes\n",
      "Regression Mean Absolute Error: 17.53 minutes\n",
      "Regression R Score: -0.0470\n",
      "Preprocessor saved to ./dep_delay_nn/year_2023_rnn\\rnn_preprocessor_2023.joblib\n",
      "\n",
      "RNN+Attention model training for 2023 complete! Total processing time: 194.73 seconds\n",
      "\n",
      "RNN+Attention model for year 2023 completed successfully!\n",
      "\n",
      "================================================================================\n",
      "Training RNN+Attention model for year 2024\n",
      "================================================================================\n",
      "\n",
      "Processing May2024.csv...\n",
      "Years found in data: [2024]\n",
      "Months found in data: {5: 649428}\n",
      "Filtered to only May data: 649428 rows\n",
      "Filtered from 649428 to 228159 rows for top 30 airports\n",
      "Removed 2994.0 cancelled flights, remaining: 225165\n",
      "Processing took: 3.29 seconds\n",
      "\n",
      "Matching weather data with flights...\n",
      "Processed 10000/225165 rows, matched 7108 flights with weather data\n",
      "Processed 20000/225165 rows, matched 14410 flights with weather data\n",
      "Processed 30000/225165 rows, matched 21856 flights with weather data\n",
      "Processed 40000/225165 rows, matched 29104 flights with weather data\n",
      "Processed 50000/225165 rows, matched 36479 flights with weather data\n",
      "Processed 60000/225165 rows, matched 43708 flights with weather data\n",
      "Processed 70000/225165 rows, matched 50965 flights with weather data\n",
      "Processed 80000/225165 rows, matched 58539 flights with weather data\n",
      "Processed 90000/225165 rows, matched 65682 flights with weather data\n",
      "Processed 100000/225165 rows, matched 73009 flights with weather data\n",
      "Processed 110000/225165 rows, matched 80433 flights with weather data\n",
      "Processed 120000/225165 rows, matched 87679 flights with weather data\n",
      "Processed 130000/225165 rows, matched 95122 flights with weather data\n",
      "Processed 140000/225165 rows, matched 102419 flights with weather data\n",
      "Processed 150000/225165 rows, matched 109622 flights with weather data\n",
      "Processed 160000/225165 rows, matched 117118 flights with weather data\n",
      "Processed 170000/225165 rows, matched 124380 flights with weather data\n",
      "Processed 180000/225165 rows, matched 131691 flights with weather data\n",
      "Processed 190000/225165 rows, matched 139073 flights with weather data\n",
      "Processed 200000/225165 rows, matched 146378 flights with weather data\n",
      "Processed 210000/225165 rows, matched 153882 flights with weather data\n",
      "Processed 220000/225165 rows, matched 161154 flights with weather data\n",
      "Processed 225165/225165 rows, matched 165140 flights with weather data\n",
      "Matched weather data for 165140 flights (73.34%)\n",
      "Weather matching took: 73.99 seconds\n",
      "\n",
      "Creating red-eye flight indicator for 2024...\n",
      "Identified 6159 red-eye flights based on departure time (0-6 AM)\n",
      "Identified 8151 red-eye flights based on arrival time (0-6 AM)\n",
      "Total identified red-eye flights: 13987 out of 225165 total flights (6.21%)\n",
      "\n",
      "Distribution of flights by departure time of day:\n",
      "  - Morning (6-12): 84574 flights (37.56%)\n",
      "  - Afternoon (12-18): 77071 flights (34.23%)\n",
      "  - Evening (18-24): 51803 flights (23.01%)\n",
      "  - Early Morning (0-6): 11717 flights (5.20%)\n",
      "\n",
      "Preparing delay data for 2024...\n",
      "\n",
      "Delay statistics:\n",
      "Delayed flights: 105538/225165 (46.87%)\n",
      "On-time or early flights: 119627/225165 (53.13%)\n",
      "\n",
      "Delay magnitude statistics:\n",
      "Mean delay: 20.52 minutes\n",
      "Median delay: 0.00 minutes\n",
      "Min delay: -31.00 minutes (negative means early departure)\n",
      "Max delay: 2232.00 minutes\n",
      "Clipped delay values above 350.00 minutes for neural network training\n",
      "Number of clipped values: 1124\n",
      "\n",
      "Delay category distribution:\n",
      "  - Very Early: 451 flights (0.20%)\n",
      "  - Early: 119176 flights (52.93%)\n",
      "  - On Time: 43825 flights (19.46%)\n",
      "  - Moderate Delay: 36901 flights (16.39%)\n",
      "  - Significant Delay: 14300 flights (6.35%)\n",
      "  - Severe Delay: 10512 flights (4.67%)\n",
      "\n",
      "Creating advanced time features for 2024...\n",
      "\n",
      "Creating advanced day features for 2024...\n",
      "\n",
      "Distribution of flights by day of week:\n",
      "  - Friday: 37632 flights (16.71%)\n",
      "  - Thursday: 37381 flights (16.60%)\n",
      "  - Wednesday: 36037 flights (16.00%)\n",
      "  - Monday: 30205 flights (13.41%)\n",
      "  - Sunday: 28931 flights (12.85%)\n",
      "  - Tuesday: 28826 flights (12.80%)\n",
      "  - Saturday: 26153 flights (11.62%)\n",
      "\n",
      "Weekend flights: 55084 (24.46%)\n",
      "Weekday flights: 170081 (75.54%)\n",
      "\n",
      "Creating advanced airport features for 2024...\n",
      "\n",
      "Creating advanced weather features for 2024...\n",
      "\n",
      "Selecting features for delay prediction for 2024...\n",
      "Using categorical features: ['TIME_BLOCK', 'DAY_NAME', 'MKT_AIRLINE', 'ORIGIN_IATA', 'DEST_IATA', 'DISTANCE_CAT']\n",
      "Using numerical features: ['DISTANCE', 'PRCP', 'EXTREME_WEATHER', 'IS_REDEYE', 'IS_WEEKEND', 'IS_MORNING_PEAK', 'IS_EVENING_PEAK', 'HOUR_SIN', 'HOUR_COS', 'HALFDAY_SIN', 'HALFDAY_COS', 'QUARTER_DAY_SIN', 'QUARTER_DAY_COS', 'DAY_SIN', 'DAY_COS', 'WEEKDAY_SIN', 'WEEKDAY_COS', 'WORKWEEK_SIN', 'WORKWEEK_COS', 'IS_MAJOR_HUB_ORIGIN', 'IS_MAJOR_HUB_DEST', 'IS_HUB_TO_HUB', 'IS_WEST_COAST_ORIGIN', 'IS_EAST_COAST_ORIGIN', 'IS_CENTRAL_ORIGIN', 'IS_WEST_COAST_DEST', 'IS_EAST_COAST_DEST', 'IS_CENTRAL_DEST', 'IS_TRANSCON', 'NORMALIZED_DISTANCE', 'LOG_DISTANCE', 'RAIN_SEVERITY', 'WEATHER_SCORE', 'HUB_WEATHER_IMPACT', 'PEAK_WEATHER_IMPACT']\n",
      "Using clipped delay values for regression to improve neural network training\n",
      "\n",
      "Preprocessing features for neural network training...\n",
      "Processed feature shape: (225165, 124)\n",
      "Training set size: (202648, 124)\n",
      "Test set size: (22517, 124)\n",
      "\n",
      "Building and training RNN+Attention neural network models...\n",
      "\n",
      "Training RNN+Attention classification model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|| 198/198 [00:02<00:00, 74.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.6294, Val Loss: 0.6099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|| 198/198 [00:02<00:00, 84.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 0.6157, Val Loss: 0.6064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|| 198/198 [00:02<00:00, 82.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 0.6116, Val Loss: 0.6050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|| 198/198 [00:02<00:00, 85.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 0.6091, Val Loss: 0.6035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|| 198/198 [00:02<00:00, 84.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 0.6066, Val Loss: 0.6022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|| 198/198 [00:02<00:00, 82.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 0.6050, Val Loss: 0.6010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|| 198/198 [00:02<00:00, 83.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 0.6027, Val Loss: 0.6004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|| 198/198 [00:02<00:00, 81.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 0.6017, Val Loss: 0.5995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|| 198/198 [00:02<00:00, 80.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 0.6002, Val Loss: 0.5994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|| 198/198 [00:02<00:00, 86.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 0.5988, Val Loss: 0.5990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|| 198/198 [00:02<00:00, 80.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 0.5979, Val Loss: 0.5988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|| 198/198 [00:02<00:00, 83.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 0.5964, Val Loss: 0.5979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|| 198/198 [00:02<00:00, 80.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Loss: 0.5954, Val Loss: 0.5969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|| 198/198 [00:02<00:00, 85.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Loss: 0.5938, Val Loss: 0.5975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|| 198/198 [00:02<00:00, 85.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Loss: 0.5929, Val Loss: 0.5959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|| 198/198 [00:02<00:00, 83.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Train Loss: 0.5920, Val Loss: 0.5963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|| 198/198 [00:02<00:00, 85.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Loss: 0.5909, Val Loss: 0.5957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|| 198/198 [00:02<00:00, 83.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Loss: 0.5900, Val Loss: 0.5956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|| 198/198 [00:02<00:00, 84.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Loss: 0.5892, Val Loss: 0.5964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|| 198/198 [00:02<00:00, 86.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Train Loss: 0.5880, Val Loss: 0.5943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|| 198/198 [00:02<00:00, 76.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Loss: 0.5871, Val Loss: 0.5956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|| 198/198 [00:02<00:00, 84.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Train Loss: 0.5863, Val Loss: 0.5956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|| 198/198 [00:02<00:00, 83.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Train Loss: 0.5854, Val Loss: 0.5944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|| 198/198 [00:02<00:00, 82.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Loss: 0.5843, Val Loss: 0.5956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|| 198/198 [00:02<00:00, 81.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Train Loss: 0.5807, Val Loss: 0.5947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|| 198/198 [00:02<00:00, 83.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Loss: 0.5787, Val Loss: 0.5949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|| 198/198 [00:02<00:00, 85.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Train Loss: 0.5783, Val Loss: 0.5952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|| 198/198 [00:02<00:00, 86.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Loss: 0.5768, Val Loss: 0.5944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|| 198/198 [00:02<00:00, 86.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Loss: 0.5740, Val Loss: 0.5947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|| 198/198 [00:02<00:00, 85.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Loss: 0.5733, Val Loss: 0.5949\n",
      "Early stopping triggered after 30 epochs\n",
      "Classification model training took: 76.78 seconds\n",
      "\n",
      "Training RNN+Attention regression model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|| 198/198 [00:02<00:00, 81.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 282.2681, Val Loss: 277.0486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|| 198/198 [00:02<00:00, 80.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 281.2665, Val Loss: 277.0605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|| 198/198 [00:02<00:00, 87.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 281.2316, Val Loss: 277.0642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|| 198/198 [00:02<00:00, 77.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 281.2075, Val Loss: 277.0618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|| 198/198 [00:02<00:00, 84.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 281.1832, Val Loss: 277.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|| 198/198 [00:02<00:00, 84.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 281.1751, Val Loss: 277.0498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|| 198/198 [00:02<00:00, 85.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 281.1611, Val Loss: 277.1065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|| 198/198 [00:02<00:00, 83.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 281.1283, Val Loss: 277.0750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|| 198/198 [00:02<00:00, 82.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 281.1221, Val Loss: 277.0810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|| 198/198 [00:02<00:00, 81.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 281.0370, Val Loss: 277.1225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|| 198/198 [00:02<00:00, 81.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 281.0006, Val Loss: 277.1855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|| 198/198 [00:02<00:00, 85.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 280.9641, Val Loss: 277.1365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|| 198/198 [00:02<00:00, 78.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Loss: 280.9303, Val Loss: 277.2553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|| 198/198 [00:02<00:00, 82.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Loss: 280.8497, Val Loss: 277.2706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|| 198/198 [00:02<00:00, 83.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Loss: 280.8234, Val Loss: 277.3394\n",
      "Early stopping triggered after 15 epochs\n",
      "Regression model training took: 38.38 seconds\n",
      "\n",
      "Evaluating classification model...\n",
      "Classification Accuracy: 68.04%\n",
      "Classification ROC AUC: 0.7449\n",
      "Warning: Class '1' not present in test results. Using default metrics.\n",
      "\n",
      "Evaluating regression model...\n",
      "Regression Mean Squared Error: 2628.27\n",
      "Regression Root Mean Squared Error: 51.27 minutes\n",
      "Regression Mean Absolute Error: 24.28 minutes\n",
      "Regression R Score: -0.0870\n",
      "Preprocessor saved to ./dep_delay_nn/year_2024_rnn\\rnn_preprocessor_2024.joblib\n",
      "\n",
      "RNN+Attention model training for 2024 complete! Total processing time: 196.57 seconds\n",
      "\n",
      "RNN+Attention model for year 2024 completed successfully!\n",
      "\n",
      "Comparing RNN_ATTENTION models across years...\n",
      "Comparison summary saved to ./dep_delay_nn/comparison_rnn_attention\\rnn_attention_delay_summary.csv\n",
      "Error creating model comparison: 'class_accuracy'\n",
      "RNN_ATTENTION model comparison completed!\n",
      "\n",
      "Year-by-Year RNN+Attention Model Training Summary:\n",
      "\n",
      "Year 2021:\n",
      "  Total flights: 171,382\n",
      "  Classification accuracy: 72.61%\n",
      "  Classification AUC: 0.7241\n",
      "  Regression RMSE: 29.14 minutes\n",
      "  Regression R: -0.0345\n",
      "  Mean delay: 6.99 minutes\n",
      "  Delay rate: 32.01%\n",
      "  Training time: 85.77 seconds\n",
      "\n",
      "Year 2022:\n",
      "  Total flights: 205,420\n",
      "  Classification accuracy: 66.33%\n",
      "  Classification AUC: 0.7188\n",
      "  Regression RMSE: 39.71 minutes\n",
      "  Regression R: -0.0657\n",
      "  Mean delay: 14.56 minutes\n",
      "  Delay rate: 44.43%\n",
      "  Training time: 107.50 seconds\n",
      "\n",
      "Year 2023:\n",
      "  Total flights: 219,176\n",
      "  Classification accuracy: 69.25%\n",
      "  Classification AUC: 0.7355\n",
      "  Regression RMSE: 38.27 minutes\n",
      "  Regression R: -0.0470\n",
      "  Mean delay: 12.35 minutes\n",
      "  Delay rate: 40.07%\n",
      "  Training time: 116.91 seconds\n",
      "\n",
      "Year 2024:\n",
      "  Total flights: 225,165\n",
      "  Classification accuracy: 68.04%\n",
      "  Classification AUC: 0.7449\n",
      "  Regression RMSE: 51.27 minutes\n",
      "  Regression R: -0.0870\n",
      "  Mean delay: 20.52 minutes\n",
      "  Delay rate: 46.87%\n",
      "  Training time: 117.31 seconds\n",
      "\n",
      "RNN+Attention model training complete! Check output directories for detailed results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
